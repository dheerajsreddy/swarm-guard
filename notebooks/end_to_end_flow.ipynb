{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-13T02:41:31.503676Z",
     "start_time": "2025-04-13T02:41:31.492211Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.data import Batch as GeoBatch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from src.dataset.astrorag_dataset import AstroRagDataset\n",
    "from src.modules.graph_encoder import UPFDGraphSageNet"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T18:51:40.017578Z",
     "start_time": "2025-04-10T18:51:40.014641Z"
    }
   },
   "cell_type": "code",
   "source": "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")",
   "id": "b243609e542f5987",
   "outputs": [],
   "execution_count": 88
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "81a52d29a54b6d21"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "907649e6370d840e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "astrorag_dataset = AstroRagDataset()",
   "id": "d635e48c4b25c33a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T18:59:36.115523Z",
     "start_time": "2025-04-10T18:59:36.106531Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DummyMultiModalDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 num_samples: int = 100,\n",
    "                 text_length: int = 20,\n",
    "                 text_model_name: str = \"bert-base-uncased\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_samples (int): Number of samples in the dataset.\n",
    "            text_length (int): Maximum token length for the text input.\n",
    "            text_model_name (str): Hugging Face model name used to initialize the tokenizer.\n",
    "        \"\"\"\n",
    "        self.num_samples = num_samples\n",
    "        self.text_length = text_length\n",
    "\n",
    "        # Initialize the tokenizer from Hugging Face\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(text_model_name)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        # -----------\n",
    "        # Graph Data\n",
    "        # -----------\n",
    "        # Create node features: 72 nodes each with 10 features, shape [72, 10]\n",
    "        x = torch.randn(72, 10, dtype=torch.float)\n",
    "\n",
    "        # Create edge_index: a tensor with shape [2, 71] representing 71 random edges\n",
    "        edge_index = torch.randint(0, 72, (2, 71), dtype=torch.long)\n",
    "\n",
    "        # Create a dummy label tensor (for binary classification) with shape [1]\n",
    "        y = torch.randint(0, 2, (1,), dtype=torch.long)\n",
    "\n",
    "        # Create a batch tensor: for a single graph, all nodes have the same batch index, here 0.\n",
    "        batch = torch.zeros(72, dtype=torch.long)\n",
    "\n",
    "        # Create a PyTorch Geometric Data object for the graph\n",
    "        graph_data = Data(x=x, edge_index=edge_index, y=y, batch=batch)\n",
    "\n",
    "        # -----------\n",
    "        # Text Data\n",
    "        # -----------\n",
    "        # Create a dummy text sample (with index for variability)\n",
    "        dummy_text = f\"This is a dummy sentence number {idx} for testing multimodal input.\"\n",
    "\n",
    "        # Tokenize using the specified tokenizer, padding/truncating to self.text_length\n",
    "        tokenized = self.tokenizer(dummy_text,\n",
    "                                   max_length=self.text_length,\n",
    "                                   padding='max_length',\n",
    "                                   truncation=True,\n",
    "                                   return_tensors='pt')\n",
    "\n",
    "        # Remove the extra batch dimension from the tokenized outputs; final shape is [text_length]\n",
    "        text_input_ids = tokenized['input_ids'].squeeze(0)\n",
    "        text_attention_mask = tokenized['attention_mask'].squeeze(0)\n",
    "\n",
    "        # Return the data and label (convert label to a scalar using .item())\n",
    "        return {\n",
    "            'text_input_ids': text_input_ids,  # Shape: [text_length]\n",
    "            'text_attention_mask': text_attention_mask,  # Shape: [text_length]\n",
    "            'graph_data': graph_data  # Graph Data object with x, edge_index, y, batch\n",
    "        }, y.item()  # Return the graph label as a scalar integer\n"
   ],
   "id": "48b1096d158b82fa",
   "outputs": [],
   "execution_count": 104
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T18:59:37.168896Z",
     "start_time": "2025-04-10T18:59:36.970068Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Specify the model name for text tokenization\n",
    "text_model_name = \"answerdotai/ModernBERT-base\"\n",
    "\n",
    "# Instantiate the dataset\n",
    "dataset = DummyMultiModalDataset(num_samples=10, text_model_name=text_model_name)\n",
    "\n",
    "# Get one sample from the dataset\n",
    "sample, label = dataset[0]\n",
    "\n",
    "print(\"Text Input IDs:\", sample['text_input_ids'])\n",
    "print(\"Text Attention Mask:\", sample['text_attention_mask'])\n",
    "print(\"Graph Data - x shape:\", sample['graph_data'].x.shape)\n",
    "print(\"Graph Data - edge_index shape:\", sample['graph_data'].edge_index.shape)\n",
    "print(\"Graph Data - batch shape:\", sample['graph_data'].batch.shape)\n",
    "print(\"Graph Data - y shape:\", sample['graph_data'].y.shape)"
   ],
   "id": "3fc1ecfb67633411",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Input IDs: tensor([50281,  1552,   310,   247, 28726,  6197,  1180,   470,   323,  5175,\n",
      "        23390, 26306,  3280,    15, 50282, 50283, 50283, 50283, 50283, 50283])\n",
      "Text Attention Mask: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0])\n",
      "Graph Data - x shape: torch.Size([72, 10])\n",
      "Graph Data - edge_index shape: torch.Size([2, 71])\n",
      "Graph Data - batch shape: torch.Size([72])\n",
      "Graph Data - y shape: torch.Size([1])\n"
     ]
    }
   ],
   "execution_count": 105
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T18:59:37.178404Z",
     "start_time": "2025-04-10T18:59:37.175978Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_pre_trained_graph_encoder(path: str, device: str = \"cpu\") -> UPFDGraphSageNet:\n",
    "    state_dict = torch.load(path)\n",
    "    model = UPFDGraphSageNet(\n",
    "        in_channels=10,\n",
    "        hidden_channels=256,\n",
    "        num_classes=2\n",
    "    )\n",
    "    model.load_state_dict(state_dict)\n",
    "    model = model.to(device)\n",
    "    return model"
   ],
   "id": "b80375e6202d9527",
   "outputs": [],
   "execution_count": 106
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T18:59:38.164128Z",
     "start_time": "2025-04-10T18:59:38.147172Z"
    }
   },
   "cell_type": "code",
   "source": [
    "state_dict = torch.load(\"/Users/navneet/git/research/swarm-guard/models/graph/graph_encoder.pth\")\n",
    "#print number of parameters in the state dict\n",
    "num_params = sum(p.numel() for p in state_dict.values())\n",
    "print(f\"Number of parameters in the state dict: {num_params / 1e6:.2f}M\")"
   ],
   "id": "b6f62bd2f8269f84",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in the state dict: 0.27M\n"
     ]
    }
   ],
   "execution_count": 107
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T18:59:39.041503Z",
     "start_time": "2025-04-10T18:59:39.028260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CrossModelAttentionBlock(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, num_heads: int, feed_forward_dim: int):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.mha_text_graph = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.mha_graph_text = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=num_heads, batch_first=True)\n",
    "\n",
    "        self.ff_text_graph_1 = nn.Linear(in_features=embedding_dim, out_features=feed_forward_dim)\n",
    "        self.ff_graph_text_1 = nn.Linear(in_features=embedding_dim, out_features=feed_forward_dim)\n",
    "\n",
    "        self.ff_text_graph_2 = nn.Linear(in_features=feed_forward_dim, out_features=embedding_dim)\n",
    "        self.ff_graph_text_2 = nn.Linear(in_features=feed_forward_dim, out_features=embedding_dim)\n",
    "\n",
    "        self.text_graph_norm = nn.LayerNorm(embedding_dim)\n",
    "        self.graph_text_norm = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "    def forward(self, text_embedding, graph_embedding):\n",
    "        # Cross-attention: text queries attend to graph keys/values, and vice versa.\n",
    "        mha_text_graph_out, _ = self.mha_text_graph(text_embedding, graph_embedding, graph_embedding)\n",
    "        mha_graph_text_out, _ = self.mha_graph_text(graph_embedding, text_embedding, text_embedding)\n",
    "\n",
    "        text_graph_out = F.relu(self.ff_text_graph_1(mha_text_graph_out))\n",
    "        graph_text_out = F.relu(self.ff_graph_text_1(mha_graph_text_out))\n",
    "\n",
    "        text_graph_out = self.ff_text_graph_2(text_graph_out)\n",
    "        graph_text_out = self.ff_graph_text_2(graph_text_out)\n",
    "\n",
    "        text_out = self.text_graph_norm(text_graph_out + mha_text_graph_out)\n",
    "        graph_out = self.graph_text_norm(graph_text_out + mha_graph_text_out)\n",
    "\n",
    "        return text_out, graph_out\n",
    "\n",
    "\n",
    "class MultiModalModelForClassification(nn.Module):\n",
    "    def __init__(self,\n",
    "                 text_encoder: nn.Module,\n",
    "                 graph_encoder: nn.Module,\n",
    "                 self_attention_heads: int,\n",
    "                 embedding_dim: int,\n",
    "                 num_cross_modal_attention_blocks: int,\n",
    "                 num_cross_modal_attention_heads: int,\n",
    "                 self_attn_ff_dim: int,\n",
    "                 num_cross_modal_attention_ff_dim: int,\n",
    "                 output_channels: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # Use the provided encoders and freeze them for PEFT.\n",
    "        self.text_encoder = text_encoder\n",
    "        for param in self.text_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.graph_encoder = graph_encoder\n",
    "        for param in self.graph_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Assuming the text encoder has a config with hidden_size.\n",
    "        self.text_embedding_size = self.text_encoder.config.hidden_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        ############ PROJECTION ############\n",
    "        self.text_projection = nn.Linear(in_features=self.text_embedding_size, out_features=embedding_dim)\n",
    "        # Adjust the in_features for the graph projection if needed.\n",
    "        self.graph_projection = nn.Linear(in_features=256, out_features=embedding_dim)\n",
    "\n",
    "        ############ SELF ATTENTION ############\n",
    "        self.text_self_attention = nn.MultiheadAttention(embed_dim=embedding_dim,\n",
    "                                                         num_heads=self_attention_heads,\n",
    "                                                         batch_first=True)\n",
    "        self.graph_self_attention = nn.MultiheadAttention(embed_dim=embedding_dim,\n",
    "                                                          num_heads=self_attention_heads,\n",
    "                                                          batch_first=True)\n",
    "        self.text_self_attention_norm = nn.LayerNorm(embedding_dim)\n",
    "        self.graph_self_attention_norm = nn.LayerNorm(embedding_dim)\n",
    "        self.text_self_attention_ff1 = nn.Linear(in_features=embedding_dim, out_features=self_attn_ff_dim)\n",
    "        self.text_self_attention_ff2 = nn.Linear(in_features=self_attn_ff_dim, out_features=embedding_dim)\n",
    "\n",
    "        self.graph_self_attention_ff1 = nn.Linear(in_features=embedding_dim, out_features=self_attn_ff_dim)\n",
    "        self.graph_self_attention_ff2 = nn.Linear(in_features=self_attn_ff_dim, out_features=embedding_dim)\n",
    "\n",
    "        self.text_self_attention_ff_norm = nn.LayerNorm(embedding_dim)\n",
    "        self.graph_self_attention_ff_norm = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        ############ CROSS MODAL ATTENTION ############\n",
    "        self.cross_modal_attention_blocks = nn.ModuleList([\n",
    "            CrossModelAttentionBlock(embedding_dim=embedding_dim,\n",
    "                                     num_heads=num_cross_modal_attention_heads,\n",
    "                                     feed_forward_dim=num_cross_modal_attention_ff_dim)\n",
    "            for _ in range(num_cross_modal_attention_blocks)\n",
    "        ])\n",
    "\n",
    "        ############ OUTPUT LAYER ############\n",
    "        self.output_pre_norm = nn.LayerNorm(embedding_dim * 2)\n",
    "        self.output_ff = nn.Linear(embedding_dim * 2, output_channels)\n",
    "\n",
    "    def forward(self, text_input_ids, text_attention_mask, graph_data):\n",
    "        text_embedding = self.text_encoder(input_ids=text_input_ids, attention_mask=text_attention_mask)[0]\n",
    "        _, node_embeddings, _ = self.graph_encoder(graph_data.x, graph_data.edge_index, graph_data.batch)\n",
    "        dense_graph_embeddings, mask = to_dense_batch(node_embeddings, graph_data.batch)\n",
    "\n",
    "        ############ PROJECTION ############\n",
    "        projected_text_embedding = self.text_projection(text_embedding)\n",
    "        projected_graph_embedding = self.graph_projection(dense_graph_embeddings)\n",
    "\n",
    "        ############ SELF ATTENTION ############\n",
    "        text_self_attn_out, _ = self.text_self_attention(projected_text_embedding,\n",
    "                                                         projected_text_embedding,\n",
    "                                                         projected_text_embedding)\n",
    "        graph_self_attn_out, _ = self.graph_self_attention(projected_graph_embedding,\n",
    "                                                           projected_graph_embedding,\n",
    "                                                           projected_graph_embedding,\n",
    "                                                           key_padding_mask=~mask)\n",
    "        text_self_attn_out = self.text_self_attention_norm(text_self_attn_out + projected_text_embedding)\n",
    "        graph_self_attn_out = self.graph_self_attention_norm(graph_self_attn_out + projected_graph_embedding)\n",
    "\n",
    "        text_ff_out = F.relu(self.text_self_attention_ff1(text_self_attn_out))\n",
    "        graph_ff_out = F.relu(self.graph_self_attention_ff1(graph_self_attn_out))\n",
    "        text_ff_out = self.text_self_attention_ff2(text_ff_out)\n",
    "        graph_ff_out = self.graph_self_attention_ff2(graph_ff_out)\n",
    "        text_ff_out = self.text_self_attention_ff_norm(text_self_attn_out + text_ff_out)\n",
    "        graph_ff_out = self.graph_self_attention_ff_norm(graph_self_attn_out + graph_ff_out)\n",
    "\n",
    "        ############ CROSS MODAL ATTENTION ############\n",
    "        projected_text_embedding, projected_graph_embedding = text_ff_out, graph_ff_out\n",
    "        for block in self.cross_modal_attention_blocks:\n",
    "            projected_text_embedding, projected_graph_embedding = block(projected_text_embedding,\n",
    "                                                                        projected_graph_embedding)\n",
    "\n",
    "        ############ OUTPUT LAYER ############\n",
    "        global_text_embedding = torch.mean(projected_text_embedding, dim=1)\n",
    "        global_graph_embedding = torch.mean(projected_graph_embedding, dim=1)\n",
    "        combined_embedding = torch.cat((global_text_embedding, global_graph_embedding), dim=-1)\n",
    "        combined_embedding = self.output_pre_norm(combined_embedding)\n",
    "        output = self.output_ff(combined_embedding)\n",
    "        return output"
   ],
   "id": "17d2b737f6bbb28b",
   "outputs": [],
   "execution_count": 108
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T18:59:41.224055Z",
     "start_time": "2025-04-10T18:59:39.856437Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text_encoder = AutoModel.from_pretrained('answerdotai/ModernBERT-base').to(device)\n",
    "graph_encoder = load_pre_trained_graph_encoder(\n",
    "    path='/Users/navneet/git/research/swarm-guard/models/graph/graph_encoder.pth',\n",
    "    device=device\n",
    ")\n",
    "model = MultiModalModelForClassification(\n",
    "    text_encoder=text_encoder,\n",
    "    graph_encoder=graph_encoder,\n",
    "    self_attention_heads=8,\n",
    "    embedding_dim=256,\n",
    "    num_cross_modal_attention_blocks=6,\n",
    "    num_cross_modal_attention_heads=8,\n",
    "    self_attn_ff_dim=512,\n",
    "    num_cross_modal_attention_ff_dim=512,\n",
    "    output_channels=2\n",
    ").to(device)"
   ],
   "id": "aa4f02e8a82436f",
   "outputs": [],
   "execution_count": 109
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T18:59:42.092854Z",
     "start_time": "2025-04-10T18:59:42.039943Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# forward pass with the sample\n",
    "sample, y = dataset[0]\n",
    "text_input_ids = sample['text_input_ids'].unsqueeze(0)\n",
    "text_attention_mask = sample['text_attention_mask'].unsqueeze(0)\n",
    "graph_data = sample['graph_data']\n",
    "#Move the data to the same device as the model\n",
    "text_input_ids = text_input_ids.to(device)\n",
    "text_attention_mask = text_attention_mask.to(device)\n",
    "graph_data.x = graph_data.x.to(device)\n",
    "graph_data.edge_index = graph_data.edge_index.to(device)\n",
    "graph_data.batch = graph_data.batch.to(device)\n",
    "# Perform a forward pass\n",
    "model(text_input_ids, text_attention_mask, graph_data)"
   ],
   "id": "35b46e442e607974",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6024, -0.0400]], device='mps:0', grad_fn=<LinearBackward0>)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 110
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T18:59:43.023192Z",
     "start_time": "2025-04-10T18:59:43.018792Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# print the number of parameters in the model in millions\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Number of parameters in the model: {num_params / 1e6:.2f}M\")\n",
    "\n",
    "# print the number of parameters in the model in millions excluding the text encoder and graph encoder\n",
    "num_params_excluding_encoders = sum(p.numel() for name, p in model.named_parameters() if\n",
    "                                    'text_encoder' not in name and 'graph_encoder' not in name)\n",
    "print(f\"Number of parameters in the model excluding encoders: {num_params_excluding_encoders / 1e6:.2f}M\")"
   ],
   "id": "ae539414b40d3583",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in the model: 156.92M\n",
      "Number of parameters in the model excluding encoders: 7.64M\n"
     ]
    }
   ],
   "execution_count": 111
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T19:01:39.676486Z",
     "start_time": "2025-04-10T19:01:39.290042Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def multimodal_collate_fn(batch):\n",
    "    # Unzip batch elements into data dictionaries and corresponding labels\n",
    "    data_dicts, labels = zip(*batch)\n",
    "\n",
    "    text_input_ids = torch.stack([d['text_input_ids'] for d in data_dicts], dim=0)\n",
    "    text_attention_mask = torch.stack([d['text_attention_mask'] for d in data_dicts], dim=0)\n",
    "\n",
    "    # Create a batched graph using GeoBatch.from_data_list\n",
    "    graph_data = GeoBatch.from_data_list([d['graph_data'] for d in data_dicts])\n",
    "\n",
    "    # Convert labels tuple (of ints) into a tensor.\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    return {\n",
    "        'text_input_ids': text_input_ids,\n",
    "        'text_attention_mask': text_attention_mask,\n",
    "        'graph_data': graph_data,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "\n",
    "crieterion = torch.nn.CrossEntropyLoss()\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=multimodal_collate_fn)\n",
    "# iterate through the data loader taking both the features and labels\n",
    "for batch in data_loader:\n",
    "    text_input_ids = batch['text_input_ids']\n",
    "    text_attention_mask = batch['text_attention_mask']\n",
    "    graph_data = batch['graph_data']\n",
    "    labels = batch['labels']\n",
    "\n",
    "    # Move the data to the same device as the model\n",
    "    text_input_ids = text_input_ids.to(device)\n",
    "    text_attention_mask = text_attention_mask.to(device)\n",
    "    graph_data.x = graph_data.x.to(device)\n",
    "    graph_data.edge_index = graph_data.edge_index.to(device)\n",
    "    graph_data.batch = graph_data.batch.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    output = model(text_input_ids, text_attention_mask, graph_data)\n",
    "    loss = crieterion(output, labels)\n",
    "\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    print(\"Labels shape:\", labels.shape)\n",
    "    print(\"Loss:\", loss.item())\n"
   ],
   "id": "ea55a52db2759d78",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 2])\n",
      "Labels shape: torch.Size([2])\n",
      "Loss: 0.7535633444786072\n",
      "Output shape: torch.Size([2, 2])\n",
      "Labels shape: torch.Size([2])\n",
      "Loss: 0.7060585618019104\n",
      "Output shape: torch.Size([2, 2])\n",
      "Labels shape: torch.Size([2])\n",
      "Loss: 0.7927145957946777\n",
      "Output shape: torch.Size([2, 2])\n",
      "Labels shape: torch.Size([2])\n",
      "Loss: 0.7272051572799683\n",
      "Output shape: torch.Size([2, 2])\n",
      "Labels shape: torch.Size([2])\n",
      "Loss: 0.38732725381851196\n"
     ]
    }
   ],
   "execution_count": 115
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a157acaf2969cf7b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
