{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-15T16:04:12.057589Z",
     "start_time": "2025-04-15T16:04:08.932488Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.data import Batch as GeoBatch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from src.dataset.astrorag_dataset import AstroturfCampaignMultiModalDataset\n",
    "from src.modules.graph_encoder import UPFDGraphSageNet"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/navneet/git/research/swarm-guard/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/navneet/git/research/swarm-guard/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T16:04:12.068591Z",
     "start_time": "2025-04-15T16:04:12.065578Z"
    }
   },
   "cell_type": "code",
   "source": "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")",
   "id": "b243609e542f5987",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T16:04:14.106184Z",
     "start_time": "2025-04-15T16:04:12.074158Z"
    }
   },
   "cell_type": "code",
   "source": [
    "astrorag_dataset = AstroturfCampaignMultiModalDataset(\n",
    "    json_dir='/Users/navneet/git/research/brag-fake-news-campaigns/dataset1/train',\n",
    "    model_id='answerdotai/ModernBERT-base')"
   ],
   "id": "d635e48c4b25c33a",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T18:59:36.115523Z",
     "start_time": "2025-04-10T18:59:36.106531Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DummyMultiModalDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 num_samples: int = 100,\n",
    "                 text_length: int = 20,\n",
    "                 text_model_name: str = \"bert-base-uncased\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_samples (int): Number of samples in the dataset.\n",
    "            text_length (int): Maximum token length for the text input.\n",
    "            text_model_name (str): Hugging Face model name used to initialize the tokenizer.\n",
    "        \"\"\"\n",
    "        self.num_samples = num_samples\n",
    "        self.text_length = text_length\n",
    "\n",
    "        # Initialize the tokenizer from Hugging Face\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(text_model_name)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        # -----------\n",
    "        # Graph Data\n",
    "        # -----------\n",
    "        # Create node features: 72 nodes each with 10 features, shape [72, 10]\n",
    "        x = torch.randn(72, 10, dtype=torch.float)\n",
    "\n",
    "        # Create edge_index: a tensor with shape [2, 71] representing 71 random edges\n",
    "        edge_index = torch.randint(0, 72, (2, 71), dtype=torch.long)\n",
    "\n",
    "        # Create a dummy label tensor (for binary classification) with shape [1]\n",
    "        y = torch.randint(0, 2, (1,), dtype=torch.long)\n",
    "\n",
    "        # Create a batch tensor: for a single graph, all nodes have the same batch index, here 0.\n",
    "        batch = torch.zeros(72, dtype=torch.long)\n",
    "\n",
    "        # Create a PyTorch Geometric Data object for the graph\n",
    "        graph_data = Data(x=x, edge_index=edge_index, y=y, batch=batch)\n",
    "\n",
    "        # -----------\n",
    "        # Text Data\n",
    "        # -----------\n",
    "        # Create a dummy text sample (with index for variability)\n",
    "        dummy_text = f\"This is a dummy sentence number {idx} for testing multimodal input.\"\n",
    "\n",
    "        # Tokenize using the specified tokenizer, padding/truncating to self.text_length\n",
    "        tokenized = self.tokenizer(dummy_text,\n",
    "                                   max_length=self.text_length,\n",
    "                                   padding='max_length',\n",
    "                                   truncation=True,\n",
    "                                   return_tensors='pt')\n",
    "\n",
    "        # Remove the extra batch dimension from the tokenized outputs; final shape is [text_length]\n",
    "        text_input_ids = tokenized['input_ids'].squeeze(0)\n",
    "        text_attention_mask = tokenized['attention_mask'].squeeze(0)\n",
    "\n",
    "        # Return the data and label (convert label to a scalar using .item())\n",
    "        return {\n",
    "            'text_input_ids': text_input_ids,  # Shape: [text_length]\n",
    "            'text_attention_mask': text_attention_mask,  # Shape: [text_length]\n",
    "            'graph_data': graph_data  # Graph Data object with x, edge_index, y, batch\n",
    "        }, y.item()  # Return the graph label as a scalar integer\n"
   ],
   "id": "48b1096d158b82fa",
   "outputs": [],
   "execution_count": 104
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1bb937a563bd901b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T16:04:17.410059Z",
     "start_time": "2025-04-15T16:04:17.396166Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Specify the model name for text tokenization\n",
    "text_model_name = \"answerdotai/ModernBERT-base\"\n",
    "\n",
    "# Instantiate the dataset\n",
    "# dataset = DummyMultiModalDataset(num_samples=10, text_model_name=text_model_name)\n",
    "\n",
    "# Get one sample from the dataset\n",
    "sample, label = astrorag_dataset[0]\n",
    "\n",
    "print(\"Text Input IDs:\", sample['text_input_ids'])\n",
    "print(\"Text Attention Mask:\", sample['text_attention_mask'])\n",
    "print(\"Graph Data - x shape:\", sample['graph_data'].x.shape)\n",
    "print(\"Graph Data - edge_index shape:\", sample['graph_data'].edge_index.shape)\n",
    "# print(\"Graph Data - batch shape:\", sample['graph_data'].batch.shape)\n",
    "print(\"Graph Data - y shape:\", sample['graph_data'].y.shape)"
   ],
   "id": "3fc1ecfb67633411",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Input IDs: tensor([50281, 12442,   267, 17680,  3551,   281,  1214, 39596, 20671,  1996,\n",
      "          436,   807,   432,  1214, 25989,   387, 23556,  9151,  2418,   273,\n",
      "          253,  6398,  5987,  1358,    85,    15,  1940,    16,    52,  2598,\n",
      "        18933,    44,    18,    54,    58,    53, 50282, 50283, 50283, 50283,\n",
      "        50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283,\n",
      "        50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283,\n",
      "        50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283,\n",
      "        50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283,\n",
      "        50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283,\n",
      "        50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283,\n",
      "        50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283,\n",
      "        50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283,\n",
      "        50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283,\n",
      "        50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283,\n",
      "        50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283,\n",
      "        50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283,\n",
      "        50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283,\n",
      "        50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283,\n",
      "        50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283,\n",
      "        50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283,\n",
      "        50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283,\n",
      "        50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283,\n",
      "        50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283,\n",
      "        50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283,\n",
      "        50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283,\n",
      "        50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283,\n",
      "        50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283,\n",
      "        50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283])\n",
      "Text Attention Mask: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Graph Data - x shape: torch.Size([19, 9])\n",
      "Graph Data - edge_index shape: torch.Size([2, 18])\n",
      "Graph Data - y shape: torch.Size([1])\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T16:04:40.333318Z",
     "start_time": "2025-04-15T16:04:40.130114Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## {\"in_channels\": 9, \"hidden_channels\": 64, \"num_classes\": 2, \"dropout\": 0.2954021195697293, \"lr\": 0.0015804240267104938, \"weight_decay\": 7.64927591337679e-06, \"batch_size\": 128, \"epochs\": 200, \"focal_alpha\": 0.3081135417724518, \"focal_gamma\": 1.3936990483465523}\n",
    "def load_pre_trained_graph_encoder(model_path: str, device: str = \"cpu\") -> UPFDGraphSageNet:\n",
    "    model_file = torch.load(model_path)\n",
    "    state_dict = model_file['model_state_dict']\n",
    "    config = model_file['config']\n",
    "    model = UPFDGraphSageNet(\n",
    "        in_channels=config['in_channels'],\n",
    "        hidden_channels=config['hidden_channels'],\n",
    "        num_classes=config['num_classes'],\n",
    "    )\n",
    "    model.load_state_dict(state_dict)\n",
    "    model = model.to(device)\n",
    "    print(f'MOdel loaded with hidden channels: {model.hidden_channels}')\n",
    "    return model\n",
    "\n",
    "\n",
    "load_pre_trained_graph_encoder(\n",
    "    model_path='/Users/navneet/git/research/swarm-guard/models/graph/graph_encoder.pth')"
   ],
   "id": "b80375e6202d9527",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOdel loaded with hidden channels: 64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "UPFDGraphSageNet(\n",
       "  (conv1): SAGEConv(9, 64, aggr=mean)\n",
       "  (norm1): LayerNorm(64, affine=True, mode=graph)\n",
       "  (conv2): SAGEConv(64, 64, aggr=mean)\n",
       "  (norm2): LayerNorm(64, affine=True, mode=graph)\n",
       "  (conv3): SAGEConv(64, 64, aggr=mean)\n",
       "  (norm3): LayerNorm(64, affine=True, mode=graph)\n",
       "  (classifier): Linear(in_features=64, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T15:24:24.475552Z",
     "start_time": "2025-04-15T15:24:24.460111Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Number of parameters in the model in millions\n",
    "num_params = sum(p.numel() for p in load_pre_trained_graph_encoder(\n",
    "    model_path='/Users/navneet/git/research/swarm-guard/models/graph/graph_encoder.pth').parameters())\n",
    "print(f\"Number of parameters in the model: {num_params / 1e6:.2f}M\")"
   ],
   "id": "3e6f13a83253a2f8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in the model: 0.02M\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T16:05:16.896972Z",
     "start_time": "2025-04-15T16:05:16.882784Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CrossModelAttentionBlock(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, num_heads: int, feed_forward_dim: int):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.mha_text_graph = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.mha_graph_text = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=num_heads, batch_first=True)\n",
    "\n",
    "        self.ff_text_graph_1 = nn.Linear(in_features=embedding_dim, out_features=feed_forward_dim)\n",
    "        self.ff_graph_text_1 = nn.Linear(in_features=embedding_dim, out_features=feed_forward_dim)\n",
    "\n",
    "        self.ff_text_graph_2 = nn.Linear(in_features=feed_forward_dim, out_features=embedding_dim)\n",
    "        self.ff_graph_text_2 = nn.Linear(in_features=feed_forward_dim, out_features=embedding_dim)\n",
    "\n",
    "        self.text_graph_norm = nn.LayerNorm(embedding_dim)\n",
    "        self.graph_text_norm = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "    def forward(self, text_embedding, graph_embedding):\n",
    "        # Cross-attention: text queries attend to graph keys/values, and vice versa.\n",
    "        mha_text_graph_out, _ = self.mha_text_graph(text_embedding, graph_embedding, graph_embedding)\n",
    "        mha_graph_text_out, _ = self.mha_graph_text(graph_embedding, text_embedding, text_embedding)\n",
    "\n",
    "        text_graph_out = F.relu(self.ff_text_graph_1(mha_text_graph_out))\n",
    "        graph_text_out = F.relu(self.ff_graph_text_1(mha_graph_text_out))\n",
    "\n",
    "        text_graph_out = self.ff_text_graph_2(text_graph_out)\n",
    "        graph_text_out = self.ff_graph_text_2(graph_text_out)\n",
    "\n",
    "        text_out = self.text_graph_norm(text_graph_out + mha_text_graph_out)\n",
    "        graph_out = self.graph_text_norm(graph_text_out + mha_graph_text_out)\n",
    "\n",
    "        return text_out, graph_out\n",
    "\n",
    "\n",
    "class MultiModalModelForClassification(nn.Module):\n",
    "    def __init__(self,\n",
    "                 text_encoder: nn.Module,\n",
    "                 graph_encoder: nn.Module,\n",
    "                 self_attention_heads: int,\n",
    "                 embedding_dim: int,\n",
    "                 num_cross_modal_attention_blocks: int,\n",
    "                 num_cross_modal_attention_heads: int,\n",
    "                 self_attn_ff_dim: int,\n",
    "                 num_cross_modal_attention_ff_dim: int,\n",
    "                 output_channels: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # Use the provided encoders and freeze them for PEFT.\n",
    "        self.text_encoder = text_encoder\n",
    "        for param in self.text_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.graph_encoder = graph_encoder\n",
    "        for param in self.graph_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Assuming the text encoder has a config with hidden_size.\n",
    "        self.text_embedding_size = self.text_encoder.config.hidden_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        ############ PROJECTION ############\n",
    "        self.text_projection = nn.Linear(in_features=self.text_embedding_size, out_features=embedding_dim)\n",
    "        # Adjust the in_features for the graph projection if needed.\n",
    "        self.graph_projection = nn.Linear(in_features=graph_encoder.hidden_channels, out_features=embedding_dim)\n",
    "\n",
    "        ############ SELF ATTENTION ############\n",
    "        self.text_self_attention = nn.MultiheadAttention(embed_dim=embedding_dim,\n",
    "                                                         num_heads=self_attention_heads,\n",
    "                                                         batch_first=True)\n",
    "        self.graph_self_attention = nn.MultiheadAttention(embed_dim=embedding_dim,\n",
    "                                                          num_heads=self_attention_heads,\n",
    "                                                          batch_first=True)\n",
    "        self.text_self_attention_norm = nn.LayerNorm(embedding_dim)\n",
    "        self.graph_self_attention_norm = nn.LayerNorm(embedding_dim)\n",
    "        self.text_self_attention_ff1 = nn.Linear(in_features=embedding_dim, out_features=self_attn_ff_dim)\n",
    "        self.text_self_attention_ff2 = nn.Linear(in_features=self_attn_ff_dim, out_features=embedding_dim)\n",
    "\n",
    "        self.graph_self_attention_ff1 = nn.Linear(in_features=embedding_dim, out_features=self_attn_ff_dim)\n",
    "        self.graph_self_attention_ff2 = nn.Linear(in_features=self_attn_ff_dim, out_features=embedding_dim)\n",
    "\n",
    "        self.text_self_attention_ff_norm = nn.LayerNorm(embedding_dim)\n",
    "        self.graph_self_attention_ff_norm = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        ############ CROSS MODAL ATTENTION ############\n",
    "        self.cross_modal_attention_blocks = nn.ModuleList([\n",
    "            CrossModelAttentionBlock(embedding_dim=embedding_dim,\n",
    "                                     num_heads=num_cross_modal_attention_heads,\n",
    "                                     feed_forward_dim=num_cross_modal_attention_ff_dim)\n",
    "            for _ in range(num_cross_modal_attention_blocks)\n",
    "        ])\n",
    "\n",
    "        ############ OUTPUT LAYER ############\n",
    "        self.output_pre_norm = nn.LayerNorm(embedding_dim * 2)\n",
    "        self.output_ff = nn.Linear(embedding_dim * 2, output_channels)\n",
    "\n",
    "    def forward(self, text_input_ids, text_attention_mask, graph_data):\n",
    "        text_embedding = self.text_encoder(input_ids=text_input_ids, attention_mask=text_attention_mask)[0]\n",
    "        _, node_embeddings, _ = self.graph_encoder(graph_data.x, graph_data.edge_index, graph_data.batch)\n",
    "        dense_graph_embeddings, mask = to_dense_batch(node_embeddings, graph_data.batch)\n",
    "\n",
    "        ############ PROJECTION ############\n",
    "        projected_text_embedding = self.text_projection(text_embedding)\n",
    "        projected_graph_embedding = self.graph_projection(dense_graph_embeddings)\n",
    "\n",
    "        ############ SELF ATTENTION ############\n",
    "        text_self_attn_out, _ = self.text_self_attention(projected_text_embedding,\n",
    "                                                         projected_text_embedding,\n",
    "                                                         projected_text_embedding)\n",
    "        graph_self_attn_out, _ = self.graph_self_attention(projected_graph_embedding,\n",
    "                                                           projected_graph_embedding,\n",
    "                                                           projected_graph_embedding,\n",
    "                                                           key_padding_mask=~mask)\n",
    "        text_self_attn_out = self.text_self_attention_norm(text_self_attn_out + projected_text_embedding)\n",
    "        graph_self_attn_out = self.graph_self_attention_norm(graph_self_attn_out + projected_graph_embedding)\n",
    "\n",
    "        text_ff_out = F.relu(self.text_self_attention_ff1(text_self_attn_out))\n",
    "        graph_ff_out = F.relu(self.graph_self_attention_ff1(graph_self_attn_out))\n",
    "        text_ff_out = self.text_self_attention_ff2(text_ff_out)\n",
    "        graph_ff_out = self.graph_self_attention_ff2(graph_ff_out)\n",
    "        text_ff_out = self.text_self_attention_ff_norm(text_self_attn_out + text_ff_out)\n",
    "        graph_ff_out = self.graph_self_attention_ff_norm(graph_self_attn_out + graph_ff_out)\n",
    "\n",
    "        ############ CROSS MODAL ATTENTION ############\n",
    "        projected_text_embedding, projected_graph_embedding = text_ff_out, graph_ff_out\n",
    "        for block in self.cross_modal_attention_blocks:\n",
    "            projected_text_embedding, projected_graph_embedding = block(projected_text_embedding,\n",
    "                                                                        projected_graph_embedding)\n",
    "\n",
    "        ############ OUTPUT LAYER ############\n",
    "        global_text_embedding = torch.mean(projected_text_embedding, dim=1)\n",
    "        global_graph_embedding = torch.mean(projected_graph_embedding, dim=1)\n",
    "        combined_embedding = torch.cat((global_text_embedding, global_graph_embedding), dim=-1)\n",
    "        combined_embedding = self.output_pre_norm(combined_embedding)\n",
    "        output = self.output_ff(combined_embedding)\n",
    "        return output"
   ],
   "id": "17d2b737f6bbb28b",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T16:05:19.824022Z",
     "start_time": "2025-04-15T16:05:18.236117Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text_encoder = AutoModel.from_pretrained('answerdotai/ModernBERT-base').to(device)\n",
    "graph_encoder = load_pre_trained_graph_encoder(\n",
    "    model_path='/Users/navneet/git/research/swarm-guard/models/graph/graph_encoder.pth',\n",
    "    device=device\n",
    ")\n",
    "model = MultiModalModelForClassification(\n",
    "    text_encoder=text_encoder,\n",
    "    graph_encoder=graph_encoder,\n",
    "    self_attention_heads=8,\n",
    "    embedding_dim=256,\n",
    "    num_cross_modal_attention_blocks=6,\n",
    "    num_cross_modal_attention_heads=8,\n",
    "    self_attn_ff_dim=512,\n",
    "    num_cross_modal_attention_ff_dim=512,\n",
    "    output_channels=2\n",
    ").to(device)"
   ],
   "id": "aa4f02e8a82436f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOdel loaded with hidden channels: 64\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T15:25:01.266560Z",
     "start_time": "2025-04-15T15:25:01.238Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# forward pass with the sample\n",
    "sample, y = astrorag_dataset[0]\n",
    "text_input_ids = sample['text_input_ids'].unsqueeze(0)\n",
    "text_attention_mask = sample['text_attention_mask'].unsqueeze(0)\n",
    "graph_data = sample['graph_data']\n",
    "#Move the data to the same device as the model\n",
    "text_input_ids = text_input_ids.to(device)\n",
    "text_attention_mask = text_attention_mask.to(device)\n",
    "graph_data.x = graph_data.x.to(device)\n",
    "graph_data.edge_index = graph_data.edge_index.to(device)\n",
    "graph_data.batch = graph_data.batch.to(device)\n",
    "# Perform a forward pass\n",
    "model(text_input_ids, text_attention_mask, graph_data)"
   ],
   "id": "35b46e442e607974",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[32], line 11\u001B[0m\n\u001B[1;32m      9\u001B[0m graph_data\u001B[38;5;241m.\u001B[39mx \u001B[38;5;241m=\u001B[39m graph_data\u001B[38;5;241m.\u001B[39mx\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     10\u001B[0m graph_data\u001B[38;5;241m.\u001B[39medge_index \u001B[38;5;241m=\u001B[39m graph_data\u001B[38;5;241m.\u001B[39medge_index\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m---> 11\u001B[0m graph_data\u001B[38;5;241m.\u001B[39mbatch \u001B[38;5;241m=\u001B[39m \u001B[43mgraph_data\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m(device)\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m# Perform a forward pass\u001B[39;00m\n\u001B[1;32m     13\u001B[0m model(text_input_ids, text_attention_mask, graph_data)\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'to'"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T15:25:11.760011Z",
     "start_time": "2025-04-15T15:25:11.753249Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# print the number of parameters in the model in millions\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Number of parameters in the model: {num_params / 1e6:.2f}M\")\n",
    "\n",
    "# print the number of parameters in the model in millions excluding the text encoder and graph encoder\n",
    "num_params_excluding_encoders = sum(p.numel() for name, p in model.named_parameters() if\n",
    "                                    'text_encoder' not in name and 'graph_encoder' not in name)\n",
    "print(f\"Number of parameters in the model excluding encoders: {num_params_excluding_encoders / 1e6:.2f}M\")"
   ],
   "id": "ae539414b40d3583",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in the model: 156.67M\n",
      "Number of parameters in the model excluding encoders: 7.64M\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T16:05:48.450952Z",
     "start_time": "2025-04-15T16:05:37.393283Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def multimodal_collate_fn(batch):\n",
    "    # Unzip batch elements into data dictionaries and corresponding labels\n",
    "    data_dicts, labels = zip(*batch)\n",
    "\n",
    "    text_input_ids = torch.stack([d['text_input_ids'] for d in data_dicts], dim=0)\n",
    "    text_attention_mask = torch.stack([d['text_attention_mask'] for d in data_dicts], dim=0)\n",
    "\n",
    "    # Create a batched graph using GeoBatch.from_data_list\n",
    "    graph_data = GeoBatch.from_data_list([d['graph_data'] for d in data_dicts])\n",
    "\n",
    "    # Convert labels tuple (of ints) into a tensor.\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    return {\n",
    "        'text_input_ids': text_input_ids,\n",
    "        'text_attention_mask': text_attention_mask,\n",
    "        'graph_data': graph_data,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "\n",
    "crieterion = torch.nn.CrossEntropyLoss()\n",
    "data_loader = torch.utils.data.DataLoader(astrorag_dataset, batch_size=2, shuffle=True,\n",
    "                                          collate_fn=multimodal_collate_fn)\n",
    "# iterate through the data loader taking both the features and labels\n",
    "for batch in data_loader:\n",
    "    text_input_ids = batch['text_input_ids']\n",
    "    text_attention_mask = batch['text_attention_mask']\n",
    "    graph_data = batch['graph_data']\n",
    "    labels = batch['labels']\n",
    "\n",
    "    # Move the data to the same device as the model\n",
    "    text_input_ids = text_input_ids.to(device)\n",
    "    text_attention_mask = text_attention_mask.to(device)\n",
    "    graph_data.x = graph_data.x.to(device)\n",
    "    graph_data.edge_index = graph_data.edge_index.to(device)\n",
    "    graph_data.batch = graph_data.batch.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    output = model(text_input_ids, text_attention_mask, graph_data)\n",
    "    loss = crieterion(output, labels)\n",
    "\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    print(\"Labels shape:\", labels.shape)\n",
    "    print(\"Loss:\", loss.item())\n"
   ],
   "id": "ea55a52db2759d78",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 2])\n",
      "Labels shape: torch.Size([2])\n",
      "Loss: 0.5210650563240051\n",
      "Output shape: torch.Size([2, 2])\n",
      "Labels shape: torch.Size([2])\n",
      "Loss: 0.46840599179267883\n",
      "Output shape: torch.Size([2, 2])\n",
      "Labels shape: torch.Size([2])\n",
      "Loss: 0.5839701294898987\n",
      "Output shape: torch.Size([2, 2])\n",
      "Labels shape: torch.Size([2])\n",
      "Loss: 0.5308350324630737\n",
      "Output shape: torch.Size([2, 2])\n",
      "Labels shape: torch.Size([2])\n",
      "Loss: 0.41162973642349243\n",
      "Output shape: torch.Size([2, 2])\n",
      "Labels shape: torch.Size([2])\n",
      "Loss: 0.7288163304328918\n",
      "Output shape: torch.Size([2, 2])\n",
      "Labels shape: torch.Size([2])\n",
      "Loss: 0.6156578063964844\n",
      "Output shape: torch.Size([2, 2])\n",
      "Labels shape: torch.Size([2])\n",
      "Loss: 0.6752355098724365\n",
      "Output shape: torch.Size([2, 2])\n",
      "Labels shape: torch.Size([2])\n",
      "Loss: 0.5649727582931519\n",
      "Output shape: torch.Size([2, 2])\n",
      "Labels shape: torch.Size([2])\n",
      "Loss: 0.7341600060462952\n",
      "Output shape: torch.Size([2, 2])\n",
      "Labels shape: torch.Size([2])\n",
      "Loss: 0.6944077014923096\n",
      "Output shape: torch.Size([2, 2])\n",
      "Labels shape: torch.Size([2])\n",
      "Loss: 0.7926101088523865\n",
      "Output shape: torch.Size([2, 2])\n",
      "Labels shape: torch.Size([2])\n",
      "Loss: 0.6792159676551819\n",
      "Output shape: torch.Size([2, 2])\n",
      "Labels shape: torch.Size([2])\n",
      "Loss: 0.6197944283485413\n",
      "Output shape: torch.Size([2, 2])\n",
      "Labels shape: torch.Size([2])\n",
      "Loss: 0.5680874586105347\n",
      "Output shape: torch.Size([2, 2])\n",
      "Labels shape: torch.Size([2])\n",
      "Loss: 0.49261394143104553\n",
      "Output shape: torch.Size([2, 2])\n",
      "Labels shape: torch.Size([2])\n",
      "Loss: 0.8131792545318604\n",
      "Output shape: torch.Size([2, 2])\n",
      "Labels shape: torch.Size([2])\n",
      "Loss: 0.40650564432144165\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 40\u001B[0m\n\u001B[1;32m     37\u001B[0m graph_data\u001B[38;5;241m.\u001B[39mbatch \u001B[38;5;241m=\u001B[39m graph_data\u001B[38;5;241m.\u001B[39mbatch\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     38\u001B[0m labels \u001B[38;5;241m=\u001B[39m labels\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m---> 40\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext_input_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtext_attention_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgraph_data\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     41\u001B[0m loss \u001B[38;5;241m=\u001B[39m crieterion(output, labels)\n\u001B[1;32m     43\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOutput shape:\u001B[39m\u001B[38;5;124m\"\u001B[39m, output\u001B[38;5;241m.\u001B[39mshape)\n",
      "File \u001B[0;32m~/git/research/swarm-guard/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/git/research/swarm-guard/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[0;32mIn[6], line 97\u001B[0m, in \u001B[0;36mMultiModalModelForClassification.forward\u001B[0;34m(self, text_input_ids, text_attention_mask, graph_data)\u001B[0m\n\u001B[1;32m     95\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, text_input_ids, text_attention_mask, graph_data):\n\u001B[1;32m     96\u001B[0m     text_embedding \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtext_encoder(input_ids\u001B[38;5;241m=\u001B[39mtext_input_ids, attention_mask\u001B[38;5;241m=\u001B[39mtext_attention_mask)[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m---> 97\u001B[0m     _, node_embeddings, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgraph_encoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgraph_data\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgraph_data\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43medge_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgraph_data\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     98\u001B[0m     dense_graph_embeddings, mask \u001B[38;5;241m=\u001B[39m to_dense_batch(node_embeddings, graph_data\u001B[38;5;241m.\u001B[39mbatch)\n\u001B[1;32m    100\u001B[0m     \u001B[38;5;66;03m############ PROJECTION ############\u001B[39;00m\n",
      "File \u001B[0;32m~/git/research/swarm-guard/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/git/research/swarm-guard/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/git/research/swarm-guard/src/modules/graph_encoder.py:36\u001B[0m, in \u001B[0;36mUPFDGraphSageNet.forward\u001B[0;34m(self, x, edge_index, batch)\u001B[0m\n\u001B[1;32m     34\u001B[0m h1 \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mrelu(h1)\n\u001B[1;32m     35\u001B[0m h1 \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mdropout(h1, p\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout, training\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining)\n\u001B[0;32m---> 36\u001B[0m h1 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnorm1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mh1\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     38\u001B[0m h2 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv2(h1, edge_index)\n\u001B[1;32m     39\u001B[0m h2 \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mrelu(h2)\n",
      "File \u001B[0;32m~/git/research/swarm-guard/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/git/research/swarm-guard/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/git/research/swarm-guard/.venv/lib/python3.9/site-packages/torch_geometric/nn/norm/layer_norm.py:81\u001B[0m, in \u001B[0;36mLayerNorm.forward\u001B[0;34m(self, x, batch, batch_size)\u001B[0m\n\u001B[1;32m     79\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgraph\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m     80\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m batch \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m---> 81\u001B[0m         x \u001B[38;5;241m=\u001B[39m x \u001B[38;5;241m-\u001B[39m \u001B[43mx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmean\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     82\u001B[0m         out \u001B[38;5;241m=\u001B[39m x \u001B[38;5;241m/\u001B[39m (x\u001B[38;5;241m.\u001B[39mstd(unbiased\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m) \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39meps)\n\u001B[1;32m     84\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a157acaf2969cf7b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
