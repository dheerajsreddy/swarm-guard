{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-18T05:40:56.370672Z",
     "start_time": "2025-04-18T05:40:53.160445Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from src.dataset.astroturf_dataset import AstroturfCampaignMultiModalDataset\n",
    "from src.modules.graph_encoder import UPFDGraphSageNet"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/navneet/git/research/swarm-guard/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/navneet/git/research/swarm-guard/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T05:40:56.381848Z",
     "start_time": "2025-04-18T05:40:56.378979Z"
    }
   },
   "cell_type": "code",
   "source": "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")",
   "id": "b243609e542f5987",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T05:40:59.321679Z",
     "start_time": "2025-04-18T05:40:57.194479Z"
    }
   },
   "cell_type": "code",
   "source": [
    "astrorag_dataset = AstroturfCampaignMultiModalDataset(\n",
    "    json_dir='/Users/navneet/git/research/brag-fake-news-campaigns/dataset1/train',\n",
    "    model_id='answerdotai/ModernBERT-base')"
   ],
   "id": "d635e48c4b25c33a",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T05:40:59.330141Z",
     "start_time": "2025-04-18T05:40:59.325943Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DummyMultiModalDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 num_samples: int = 100,\n",
    "                 text_length: int = 20,\n",
    "                 text_model_name: str = \"bert-base-uncased\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_samples (int): Number of samples in the dataset.\n",
    "            text_length (int): Maximum token length for the text input.\n",
    "            text_model_name (str): Hugging Face model name used to initialize the tokenizer.\n",
    "        \"\"\"\n",
    "        self.num_samples = num_samples\n",
    "        self.text_length = text_length\n",
    "\n",
    "        # Initialize the tokenizer from Hugging Face\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(text_model_name)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        # -----------\n",
    "        # Graph Data\n",
    "        # -----------\n",
    "        # Create node features: 72 nodes each with 10 features, shape [72, 10]\n",
    "        x = torch.randn(72, 10, dtype=torch.float)\n",
    "\n",
    "        # Create edge_index: a tensor with shape [2, 71] representing 71 random edges\n",
    "        edge_index = torch.randint(0, 72, (2, 71), dtype=torch.long)\n",
    "\n",
    "        # Create a dummy label tensor (for binary classification) with shape [1]\n",
    "        y = torch.randint(0, 2, (1,), dtype=torch.long)\n",
    "\n",
    "        # Create a batch tensor: for a single graph, all nodes have the same batch index, here 0.\n",
    "        batch = torch.zeros(72, dtype=torch.long)\n",
    "\n",
    "        # Create a PyTorch Geometric Data object for the graph\n",
    "        graph_data = Data(x=x, edge_index=edge_index, y=y, batch=batch)\n",
    "\n",
    "        # -----------\n",
    "        # Text Data\n",
    "        # -----------\n",
    "        # Create a dummy text sample (with index for variability)\n",
    "        dummy_text = f\"This is a dummy sentence number {idx} for testing multimodal input.\"\n",
    "\n",
    "        # Tokenize using the specified tokenizer, padding/truncating to self.text_length\n",
    "        tokenized = self.tokenizer(dummy_text,\n",
    "                                   max_length=self.text_length,\n",
    "                                   padding='max_length',\n",
    "                                   truncation=True,\n",
    "                                   return_tensors='pt')\n",
    "\n",
    "        # Remove the extra batch dimension from the tokenized outputs; final shape is [text_length]\n",
    "        text_input_ids = tokenized['input_ids'].squeeze(0)\n",
    "        text_attention_mask = tokenized['attention_mask'].squeeze(0)\n",
    "\n",
    "        # Return the data and label (convert label to a scalar using .item())\n",
    "        return {\n",
    "            'text_input_ids': text_input_ids,  # Shape: [text_length]\n",
    "            'text_attention_mask': text_attention_mask,  # Shape: [text_length]\n",
    "            'graph_data': graph_data  # Graph Data object with x, edge_index, y, batch\n",
    "        }, y.item()  # Return the graph label as a scalar integer\n"
   ],
   "id": "48b1096d158b82fa",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T05:40:59.883684Z",
     "start_time": "2025-04-18T05:40:59.881889Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "1bb937a563bd901b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T05:41:00.128309Z",
     "start_time": "2025-04-18T05:41:00.117926Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Specify the model name for text tokenization\n",
    "text_model_name = \"answerdotai/ModernBERT-base\"\n",
    "\n",
    "# Instantiate the dataset\n",
    "# dataset = DummyMultiModalDataset(num_samples=10, text_model_name=text_model_name)\n",
    "\n",
    "# Get one sample from the dataset\n",
    "sample, label = astrorag_dataset[0]\n",
    "\n",
    "print(\"Text Input IDs:\", sample['text_input_ids'])\n",
    "print(\"Text Attention Mask:\", sample['text_attention_mask'])\n",
    "print(\"Graph Data - x shape:\", sample['graph_data'].x.shape)\n",
    "print(\"Graph Data - edge_index shape:\", sample['graph_data'].edge_index.shape)\n",
    "# print(\"Graph Data - batch shape:\", sample['graph_data'].batch.shape)\n",
    "print(\"Graph Data - y shape:\", sample['graph_data'].y.shape)"
   ],
   "id": "3fc1ecfb67633411",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Input IDs: tensor([50281, 12442,   267, 17680,  3551,   281,  1214, 39596, 20671,  1996,\n",
      "          436,   807,   432,  1214, 25989,   387, 23556,  9151,  2418,   273,\n",
      "          253,  6398,  5987,  1358,    85,    15,  1940,    16,    52,  2598,\n",
      "        18933,    44,    18,    54,    58,    53, 50282, 50283, 50283, 50283,\n",
      "        50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283,\n",
      "        50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283,\n",
      "        50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283,\n",
      "        50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283,\n",
      "        50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283,\n",
      "        50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283,\n",
      "        50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283,\n",
      "        50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283,\n",
      "        50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283,\n",
      "        50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283,\n",
      "        50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283,\n",
      "        50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283,\n",
      "        50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283,\n",
      "        50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283,\n",
      "        50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283,\n",
      "        50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283,\n",
      "        50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283,\n",
      "        50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283,\n",
      "        50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283,\n",
      "        50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283,\n",
      "        50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283,\n",
      "        50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283,\n",
      "        50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283,\n",
      "        50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283])\n",
      "Text Attention Mask: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Graph Data - x shape: torch.Size([19, 9])\n",
      "Graph Data - edge_index shape: torch.Size([2, 18])\n",
      "Graph Data - y shape: torch.Size([1])\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T05:41:03.639063Z",
     "start_time": "2025-04-18T05:41:03.529202Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## {\"in_channels\": 9, \"hidden_channels\": 64, \"num_classes\": 2, \"dropout\": 0.2954021195697293, \"lr\": 0.0015804240267104938, \"weight_decay\": 7.64927591337679e-06, \"batch_size\": 128, \"epochs\": 200, \"focal_alpha\": 0.3081135417724518, \"focal_gamma\": 1.3936990483465523}\n",
    "def load_pre_trained_graph_encoder(model_path: str, device: str = \"cpu\") -> UPFDGraphSageNet:\n",
    "    model_file = torch.load(model_path)\n",
    "    state_dict = model_file['model_state_dict']\n",
    "    config = model_file['config']\n",
    "    model = UPFDGraphSageNet(\n",
    "        in_channels=config['in_channels'],\n",
    "        hidden_channels=config['hidden_channels'],\n",
    "        num_classes=config['num_classes'],\n",
    "    )\n",
    "    model.load_state_dict(state_dict)\n",
    "    model = model.to(device)\n",
    "    print(f'MOdel loaded with hidden channels: {model.hidden_channels}')\n",
    "    return model\n",
    "\n",
    "\n",
    "load_pre_trained_graph_encoder(\n",
    "    model_path='/Users/navneet/git/research/swarm-guard/models/graph/graph_encoder.pth')"
   ],
   "id": "b80375e6202d9527",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOdel loaded with hidden channels: 64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "UPFDGraphSageNet(\n",
       "  (conv1): SAGEConv(9, 64, aggr=mean)\n",
       "  (norm1): LayerNorm(64, affine=True, mode=graph)\n",
       "  (conv2): SAGEConv(64, 64, aggr=mean)\n",
       "  (norm2): LayerNorm(64, affine=True, mode=graph)\n",
       "  (conv3): SAGEConv(64, 64, aggr=mean)\n",
       "  (norm3): LayerNorm(64, affine=True, mode=graph)\n",
       "  (classifier): Linear(in_features=64, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T05:41:04.452806Z",
     "start_time": "2025-04-18T05:41:04.436335Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Number of parameters in the model in millions\n",
    "num_params = sum(p.numel() for p in load_pre_trained_graph_encoder(\n",
    "    model_path='/Users/navneet/git/research/swarm-guard/models/graph/graph_encoder.pth').parameters())\n",
    "print(f\"Number of parameters in the model: {num_params / 1e6:.2f}M\")"
   ],
   "id": "3e6f13a83253a2f8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOdel loaded with hidden channels: 64\n",
      "Number of parameters in the model: 0.02M\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T05:54:40.806849Z",
     "start_time": "2025-04-18T05:54:40.787343Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CrossModelAttentionBlock(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, num_heads: int, feed_forward_dim: int):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.mha = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.ff_1 = nn.Linear(in_features=embedding_dim, out_features=feed_forward_dim)\n",
    "        self.ff_2 = nn.Linear(in_features=feed_forward_dim, out_features=embedding_dim)\n",
    "        self.norm = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "    def forward(self, embedding_a, embedding_b):\n",
    "        mha_out, _ = self.mha(embedding_a, embedding_b, embedding_b)\n",
    "        out = F.relu(self.ff_1(mha_out))\n",
    "        out = self.ff_2(out)\n",
    "        final_out = self.norm(out + mha_out)\n",
    "        return final_out\n",
    "\n",
    "\n",
    "class MultiModalModelForClassification(nn.Module):\n",
    "    def __init__(self,\n",
    "                 text_encoder: nn.Module,\n",
    "                 graph_encoder: nn.Module,\n",
    "                 self_attention_heads: int,\n",
    "                 embedding_dim: int,\n",
    "                 num_cross_modal_attention_blocks: int,\n",
    "                 num_cross_modal_attention_heads: int,\n",
    "                 self_attn_ff_dim: int,\n",
    "                 num_cross_modal_attention_ff_dim: int,\n",
    "                 output_channels: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # Use the provided encoders and freeze them for PEFT.\n",
    "        self.text_encoder = text_encoder\n",
    "        for param in self.text_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.graph_encoder = graph_encoder\n",
    "        for param in self.graph_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Assuming the text encoder has a config with hidden_size.\n",
    "        self.text_embedding_size = self.text_encoder.config.hidden_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        ############ PROJECTION ############\n",
    "        self.text_projection = nn.Linear(in_features=self.text_embedding_size, out_features=embedding_dim)\n",
    "        # Adjust the in_features for the graph projection if needed.\n",
    "        self.graph_projection = nn.Linear(in_features=graph_encoder.hidden_channels, out_features=embedding_dim)\n",
    "\n",
    "        ############ SELF ATTENTION ############\n",
    "        self.text_self_attention = nn.MultiheadAttention(embed_dim=embedding_dim,\n",
    "                                                         num_heads=self_attention_heads,\n",
    "                                                         batch_first=True)\n",
    "        self.graph_self_attention = nn.MultiheadAttention(embed_dim=embedding_dim,\n",
    "                                                          num_heads=self_attention_heads,\n",
    "                                                          batch_first=True)\n",
    "        self.text_self_attention_norm = nn.LayerNorm(embedding_dim)\n",
    "        self.graph_self_attention_norm = nn.LayerNorm(embedding_dim)\n",
    "        self.text_self_attention_ff1 = nn.Linear(in_features=embedding_dim, out_features=self_attn_ff_dim)\n",
    "        self.text_self_attention_ff2 = nn.Linear(in_features=self_attn_ff_dim, out_features=embedding_dim)\n",
    "\n",
    "        self.graph_self_attention_ff1 = nn.Linear(in_features=embedding_dim, out_features=self_attn_ff_dim)\n",
    "        self.graph_self_attention_ff2 = nn.Linear(in_features=self_attn_ff_dim, out_features=embedding_dim)\n",
    "\n",
    "        self.text_self_attention_ff_norm = nn.LayerNorm(embedding_dim)\n",
    "        self.graph_self_attention_ff_norm = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        ############ CROSS MODAL ATTENTION ############\n",
    "        self.cross_modal_attention_blocks = nn.ModuleList([\n",
    "            CrossModelAttentionBlock(embedding_dim=embedding_dim,\n",
    "                                     num_heads=num_cross_modal_attention_heads,\n",
    "                                     feed_forward_dim=num_cross_modal_attention_ff_dim)\n",
    "            for _ in range(num_cross_modal_attention_blocks)\n",
    "        ])\n",
    "\n",
    "        ############ OUTPUT LAYER ############\n",
    "\n",
    "        # Gated Fusion\n",
    "        self.gate_fc = nn.Linear(embedding_dim * 2, 2)\n",
    "        self.post_fusion_norm = nn.LayerNorm(embedding_dim)\n",
    "        self.classifier = nn.Linear(embedding_dim, output_channels)\n",
    "\n",
    "    def forward(self, text_input_ids, text_attention_mask, graph_data):\n",
    "        text_embedding = self.text_encoder(input_ids=text_input_ids, attention_mask=text_attention_mask)[0]\n",
    "        _, node_embeddings, _ = self.graph_encoder(graph_data.x, graph_data.edge_index, graph_data.batch)\n",
    "        dense_graph_embeddings, mask = to_dense_batch(node_embeddings, graph_data.batch)\n",
    "\n",
    "        ############ PROJECTION ############\n",
    "        projected_text_embedding = self.text_projection(text_embedding)\n",
    "        projected_graph_embedding = self.graph_projection(dense_graph_embeddings)\n",
    "\n",
    "        ############ SELF ATTENTION ############\n",
    "        text_self_attn_out, _ = self.text_self_attention(projected_text_embedding,\n",
    "                                                         projected_text_embedding,\n",
    "                                                         projected_text_embedding)\n",
    "        graph_self_attn_out, _ = self.graph_self_attention(projected_graph_embedding,\n",
    "                                                           projected_graph_embedding,\n",
    "                                                           projected_graph_embedding,\n",
    "                                                           key_padding_mask=~mask)\n",
    "        text_self_attn_out = self.text_self_attention_norm(text_self_attn_out + projected_text_embedding)\n",
    "        graph_self_attn_out = self.graph_self_attention_norm(graph_self_attn_out + projected_graph_embedding)\n",
    "\n",
    "        text_ff_out = F.relu(self.text_self_attention_ff1(text_self_attn_out))\n",
    "        graph_ff_out = F.relu(self.graph_self_attention_ff1(graph_self_attn_out))\n",
    "        text_ff_out = self.text_self_attention_ff2(text_ff_out)\n",
    "        graph_ff_out = self.graph_self_attention_ff2(graph_ff_out)\n",
    "        text_ff_out = self.text_self_attention_ff_norm(text_self_attn_out + text_ff_out)\n",
    "        graph_ff_out = self.graph_self_attention_ff_norm(graph_self_attn_out + graph_ff_out)\n",
    "\n",
    "        ############ CROSS MODAL ATTENTION ############\n",
    "        projected_text_embedding, projected_graph_embedding = text_ff_out, graph_ff_out\n",
    "        for block in self.cross_modal_attention_blocks:\n",
    "            projected_text_embedding_new = block(projected_text_embedding, projected_graph_embedding)\n",
    "            projected_graph_embedding_new = block(projected_graph_embedding, projected_text_embedding)\n",
    "            projected_text_embedding, projected_graph_embedding = (projected_text_embedding_new,\n",
    "                                                                   projected_graph_embedding_new)\n",
    "\n",
    "        ############ OUTPUT LAYER ############\n",
    "        global_text_embedding = torch.mean(projected_text_embedding, dim=1)\n",
    "        global_graph_embedding = torch.mean(projected_graph_embedding, dim=1)\n",
    "\n",
    "        gated_out = self.gate_fc(torch.cat((global_text_embedding, global_graph_embedding), dim=-1))\n",
    "        gates = F.softmax(gated_out, dim=-1)\n",
    "        alpha, beta = gates[:, 0:1], gates[:, 1:2]\n",
    "        fused_embedding = (alpha * global_text_embedding) + (beta * global_graph_embedding)\n",
    "        fused_embedding = self.post_fusion_norm(fused_embedding)\n",
    "\n",
    "        logits = self.classifier(fused_embedding)\n",
    "        return logits"
   ],
   "id": "17d2b737f6bbb28b",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T05:54:44.274881Z",
     "start_time": "2025-04-18T05:54:42.602157Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text_encoder = AutoModel.from_pretrained('answerdotai/ModernBERT-base').to(device)\n",
    "graph_encoder = load_pre_trained_graph_encoder(\n",
    "    model_path='/Users/navneet/git/research/swarm-guard/models/graph/graph_encoder.pth',\n",
    "    device=device\n",
    ")\n",
    "model = MultiModalModelForClassification(\n",
    "    text_encoder=text_encoder,\n",
    "    graph_encoder=graph_encoder,\n",
    "    self_attention_heads=8,\n",
    "    embedding_dim=256,\n",
    "    num_cross_modal_attention_blocks=6,\n",
    "    num_cross_modal_attention_heads=8,\n",
    "    self_attn_ff_dim=512,\n",
    "    num_cross_modal_attention_ff_dim=512,\n",
    "    output_channels=2\n",
    ").to(device)"
   ],
   "id": "aa4f02e8a82436f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOdel loaded with hidden channels: 64\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T05:54:44.497966Z",
     "start_time": "2025-04-18T05:54:44.401433Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# forward pass with the sample\n",
    "sample, y = astrorag_dataset[0]\n",
    "text_input_ids = sample['text_input_ids'].unsqueeze(0)\n",
    "text_attention_mask = sample['text_attention_mask'].unsqueeze(0)\n",
    "graph_data = sample['graph_data']\n",
    "#Move the data to the same device as the model\n",
    "text_input_ids = text_input_ids.to(device)\n",
    "text_attention_mask = text_attention_mask.to(device)\n",
    "graph_data.x = graph_data.x.to(device)\n",
    "graph_data.edge_index = graph_data.edge_index.to(device)\n",
    "# graph_data.batch = graph_data.batch.to(device)\n",
    "# Perform a forward pass\n",
    "model(text_input_ids, text_attention_mask, graph_data)"
   ],
   "id": "35b46e442e607974",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7697, -0.0100]], device='mps:0', grad_fn=<LinearBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T05:45:10.812087Z",
     "start_time": "2025-04-18T05:45:10.806715Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# print the number of parameters in the model in millions\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Number of parameters in the model: {num_params / 1e6:.2f}M\")\n",
    "\n",
    "# print the number of parameters in the model in millions excluding the text encoder and graph encoder\n",
    "num_params_excluding_encoders = sum(p.numel() for name, p in model.named_parameters() if\n",
    "                                    'text_encoder' not in name and 'graph_encoder' not in name)\n",
    "print(f\"Number of parameters in the model excluding encoders: {num_params_excluding_encoders / 1e6:.2f}M\")"
   ],
   "id": "ae539414b40d3583",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in the model: 153.46M\n",
      "Number of parameters in the model excluding encoders: 4.43M\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T05:55:09.270656Z",
     "start_time": "2025-04-18T05:55:08.579095Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def multimodal_collate_fn(batch):\n",
    "    # Unzip batch elements into data dictionaries and corresponding labels\n",
    "    data_dicts, labels = zip(*batch)\n",
    "\n",
    "    text_input_ids = torch.stack([d['text_input_ids'] for d in data_dicts], dim=0)\n",
    "    text_attention_mask = torch.stack([d['text_attention_mask'] for d in data_dicts], dim=0)\n",
    "\n",
    "    # Create a batched graph using GeoBatch.from_data_list\n",
    "    graph_data = Batch.from_data_list([d['graph_data'] for d in data_dicts])\n",
    "\n",
    "    # Convert labels tuple (of ints) into a tensor.\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    return {\n",
    "        'text_input_ids': text_input_ids,\n",
    "        'text_attention_mask': text_attention_mask,\n",
    "        'graph_data': graph_data,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "\n",
    "crieterion = torch.nn.CrossEntropyLoss()\n",
    "data_loader = torch.utils.data.DataLoader(astrorag_dataset, batch_size=2, shuffle=True,\n",
    "                                          collate_fn=multimodal_collate_fn)\n",
    "# iterate through the data loader taking both the features and labels\n",
    "for batch in data_loader:\n",
    "    text_input_ids = batch['text_input_ids']\n",
    "    text_attention_mask = batch['text_attention_mask']\n",
    "    graph_data = batch['graph_data']\n",
    "    labels = batch['labels']\n",
    "\n",
    "    # Move the data to the same device as the model\n",
    "    text_input_ids = text_input_ids.to(device)\n",
    "    text_attention_mask = text_attention_mask.to(device)\n",
    "    graph_data.x = graph_data.x.to(device)\n",
    "    graph_data.edge_index = graph_data.edge_index.to(device)\n",
    "    graph_data.batch = graph_data.batch.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    output = model(text_input_ids, text_attention_mask, graph_data)\n",
    "    loss = crieterion(output, labels)\n",
    "\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    print(\"Labels shape:\", labels.shape)\n",
    "    print(\"Loss:\", loss.item())\n",
    "    break"
   ],
   "id": "a157acaf2969cf7b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 2])\n",
      "Labels shape: torch.Size([2])\n",
      "Loss: 0.976736307144165\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "76ab5e47d8018c1d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
