{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-21T23:18:15.996952Z",
     "start_time": "2025-04-21T23:18:12.679098Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Batch\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from transformers import AutoModel\n",
    "\n",
    "from src.dataset.astroturf_dataset import AstroturfCampaignMultiModalDataset\n",
    "from src.modules.graph_encoder import UPFDGraphSageNet"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/navneet/git/research/swarm-guard/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/navneet/git/research/swarm-guard/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T23:18:16.101892Z",
     "start_time": "2025-04-21T23:18:16.013291Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CrossModelAttentionBlock(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, num_heads: int, feed_forward_dim: int):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.mha = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.ff_1 = nn.Linear(in_features=embedding_dim, out_features=feed_forward_dim)\n",
    "        self.ff_2 = nn.Linear(in_features=feed_forward_dim, out_features=embedding_dim)\n",
    "        self.norm = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "    def forward(self, embedding_a, embedding_b):\n",
    "        mha_out, _ = self.mha(embedding_a, embedding_b, embedding_b)\n",
    "        out = F.relu(self.ff_1(mha_out))\n",
    "        out = self.ff_2(out)\n",
    "        final_out = self.norm(out + mha_out)\n",
    "        return final_out\n",
    "\n",
    "\n",
    "class MultiModalModelForClassification(nn.Module):\n",
    "    def __init__(self,\n",
    "                 text_encoder: nn.Module,\n",
    "                 graph_encoder: nn.Module,\n",
    "                 vision_encoder: nn.Module,\n",
    "                 self_attention_heads: int,\n",
    "                 embedding_dim: int,\n",
    "                 num_cross_modal_attention_blocks: int,\n",
    "                 num_cross_modal_attention_heads: int,\n",
    "                 self_attn_ff_dim: int,\n",
    "                 num_cross_modal_attention_ff_dim: int,\n",
    "                 output_channels: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # Use the provided encoders and freeze them for PEFT.\n",
    "        self.text_encoder = text_encoder\n",
    "        for param in self.text_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.graph_encoder = graph_encoder\n",
    "        for param in self.graph_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.vision_encoder = vision_encoder\n",
    "        for param in self.vision_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Assuming the text encoder has a config with hidden_size.\n",
    "        self.text_embedding_size = self.text_encoder.config.hidden_size\n",
    "\n",
    "        # Assuming the vision encoder (Vision transformer) has a config with hidden_size.\n",
    "        self.vision_embedding_size = self.vision_encoder.config.hidden_size\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        ############ PROJECTION ############\n",
    "        self.text_projection = nn.Linear(in_features=self.text_embedding_size, out_features=embedding_dim)\n",
    "        # Adjust the in_features for the graph projection if needed.\n",
    "        self.graph_projection = nn.Linear(in_features=graph_encoder.hidden_channels, out_features=embedding_dim)\n",
    "\n",
    "        self.vision_projection = nn.Linear(in_features=self.vision_embedding_size, out_features=embedding_dim)\n",
    "\n",
    "        ############ SELF ATTENTION ############\n",
    "        self.text_self_attention = nn.MultiheadAttention(embed_dim=embedding_dim,\n",
    "                                                         num_heads=self_attention_heads,\n",
    "                                                         batch_first=True)\n",
    "        self.graph_self_attention = nn.MultiheadAttention(embed_dim=embedding_dim,\n",
    "                                                          num_heads=self_attention_heads,\n",
    "                                                          batch_first=True)\n",
    "        self.vision_self_attention = nn.MultiheadAttention(embed_dim=embedding_dim,\n",
    "                                                           num_heads=self_attention_heads,\n",
    "                                                           batch_first=True)\n",
    "        self.text_self_attention_norm = nn.LayerNorm(embedding_dim)\n",
    "        self.graph_self_attention_norm = nn.LayerNorm(embedding_dim)\n",
    "        self.vision_self_attention_norm = nn.LayerNorm(embedding_dim)\n",
    "        self.text_self_attention_ff1 = nn.Linear(in_features=embedding_dim, out_features=self_attn_ff_dim)\n",
    "        self.text_self_attention_ff2 = nn.Linear(in_features=self_attn_ff_dim, out_features=embedding_dim)\n",
    "\n",
    "        self.graph_self_attention_ff1 = nn.Linear(in_features=embedding_dim, out_features=self_attn_ff_dim)\n",
    "        self.graph_self_attention_ff2 = nn.Linear(in_features=self_attn_ff_dim, out_features=embedding_dim)\n",
    "\n",
    "        self.vision_self_attention_ff1 = nn.Linear(in_features=embedding_dim, out_features=self_attn_ff_dim)\n",
    "        self.vision_self_attention_ff2 = nn.Linear(in_features=self_attn_ff_dim, out_features=embedding_dim)\n",
    "\n",
    "        self.text_self_attention_ff_norm = nn.LayerNorm(embedding_dim)\n",
    "        self.graph_self_attention_ff_norm = nn.LayerNorm(embedding_dim)\n",
    "        self.vision_self_attention_ff_norm = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        ############ CROSS MODAL ATTENTION ############\n",
    "        self.cross_modal_attention_blocks = nn.ModuleList([\n",
    "            CrossModelAttentionBlock(embedding_dim=embedding_dim,\n",
    "                                     num_heads=num_cross_modal_attention_heads,\n",
    "                                     feed_forward_dim=num_cross_modal_attention_ff_dim)\n",
    "            for _ in range(num_cross_modal_attention_blocks)\n",
    "        ])\n",
    "\n",
    "        ############ OUTPUT LAYER ############\n",
    "\n",
    "        # Gated Fusion\n",
    "        self.gate_fc = nn.Linear(embedding_dim * 3, 3)\n",
    "        self.post_fusion_norm = nn.LayerNorm(embedding_dim)\n",
    "        self.classifier = nn.Linear(embedding_dim, output_channels)\n",
    "\n",
    "    def forward(self, text_input_ids, text_attention_mask, graph_data, pixel_values):\n",
    "        text_embedding = self.text_encoder(input_ids=text_input_ids, attention_mask=text_attention_mask)[0]\n",
    "        _, node_embeddings, _ = self.graph_encoder(graph_data.x, graph_data.edge_index, graph_data.batch)\n",
    "        dense_graph_embeddings, mask = to_dense_batch(node_embeddings, graph_data.batch)\n",
    "        vision_embedding = self.vision_encoder(pixel_values=pixel_values).last_hidden_state\n",
    "\n",
    "        ############ PROJECTION ############\n",
    "        projected_text_embedding = self.text_projection(text_embedding)\n",
    "        projected_graph_embedding = self.graph_projection(dense_graph_embeddings)\n",
    "        projected_vision_embedding = self.vision_projection(vision_embedding)\n",
    "\n",
    "        ############ SELF ATTENTION ############\n",
    "        text_self_attn_out, _ = self.text_self_attention(projected_text_embedding,\n",
    "                                                         projected_text_embedding,\n",
    "                                                         projected_text_embedding)\n",
    "        graph_self_attn_out, _ = self.graph_self_attention(projected_graph_embedding,\n",
    "                                                           projected_graph_embedding,\n",
    "                                                           projected_graph_embedding,\n",
    "                                                           key_padding_mask=~mask)\n",
    "        vision_self_attn_out, _ = self.vision_self_attention(projected_vision_embedding,\n",
    "                                                             projected_vision_embedding,\n",
    "                                                             projected_vision_embedding)\n",
    "        text_self_attn_out = self.text_self_attention_norm(text_self_attn_out + projected_text_embedding)\n",
    "        graph_self_attn_out = self.graph_self_attention_norm(graph_self_attn_out + projected_graph_embedding)\n",
    "        vision_self_attn_out = self.vision_self_attention_norm(vision_self_attn_out + projected_vision_embedding)\n",
    "\n",
    "        text_ff_out = F.relu(self.text_self_attention_ff1(text_self_attn_out))\n",
    "        graph_ff_out = F.relu(self.graph_self_attention_ff1(graph_self_attn_out))\n",
    "        vision_ff_out = F.relu(self.vision_self_attention_ff1(vision_self_attn_out))\n",
    "        text_ff_out = self.text_self_attention_ff2(text_ff_out)\n",
    "        graph_ff_out = self.graph_self_attention_ff2(graph_ff_out)\n",
    "        vision_ff_out = self.vision_self_attention_ff2(vision_ff_out)\n",
    "        text_ff_out = self.text_self_attention_ff_norm(text_self_attn_out + text_ff_out)\n",
    "        graph_ff_out = self.graph_self_attention_ff_norm(graph_self_attn_out + graph_ff_out)\n",
    "        vision_ff_out = self.vision_self_attention_ff_norm(vision_self_attn_out + vision_ff_out)\n",
    "\n",
    "        ############ CROSS MODAL ATTENTION ############\n",
    "        projected_text_embedding, projected_graph_embedding, projected_vision_embedding = text_ff_out, graph_ff_out, vision_ff_out\n",
    "        for block in self.cross_modal_attention_blocks:\n",
    "            projected_text_embedding_new = block(projected_text_embedding, projected_graph_embedding)\n",
    "            projected_graph_embedding_new = block(projected_graph_embedding, projected_text_embedding)\n",
    "            projected_text_embedding_new = self.text_self_attention_ff_norm(\n",
    "                projected_text_embedding + projected_text_embedding_new)\n",
    "            projected_text_embedding, projected_graph_embedding, projected_vision_embedding = (\n",
    "                projected_text_embedding_new,\n",
    "                projected_graph_embedding_new,\n",
    "                projected_vision_embedding)\n",
    "\n",
    "        ############ OUTPUT LAYER ############\n",
    "        global_text_embedding = torch.mean(projected_text_embedding, dim=1)\n",
    "        global_graph_embedding = torch.mean(projected_graph_embedding, dim=1)\n",
    "        global_vision_embedding = torch.mean(projected_vision_embedding, dim=1)\n",
    "        gated_out = self.gate_fc(\n",
    "            torch.cat((global_text_embedding, global_graph_embedding, global_vision_embedding), dim=-1))\n",
    "        gates = F.softmax(gated_out, dim=-1)\n",
    "        alpha, beta, gamma = gates[:, 0:1], gates[:, 1:2], gates[:, 2:3]\n",
    "        fused_embedding = (alpha * global_text_embedding) + (beta * global_graph_embedding) + (\n",
    "                gamma * global_vision_embedding)\n",
    "        fused_embedding = self.post_fusion_norm(fused_embedding)\n",
    "\n",
    "        logits = self.classifier(fused_embedding)\n",
    "        return logits"
   ],
   "id": "9493a9989d208600",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T23:18:16.154804Z",
     "start_time": "2025-04-21T23:18:16.153Z"
    }
   },
   "cell_type": "code",
   "source": "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")",
   "id": "b243609e542f5987",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T23:18:18.667883Z",
     "start_time": "2025-04-21T23:18:16.160925Z"
    }
   },
   "cell_type": "code",
   "source": [
    "astrorag_dataset = AstroturfCampaignMultiModalDataset(\n",
    "    json_dir='/Users/navneet/git/research/swarm-guard/dataset1/train/graphs',\n",
    "    image_dir='/Users/navneet/git/research/swarm-guard/dataset1/train/images',\n",
    "    text_model_id='answerdotai/ModernBERT-base',\n",
    "    vision_model_id='google/vit-base-patch16-224',\n",
    ")"
   ],
   "id": "d635e48c4b25c33a",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T23:18:18.701878Z",
     "start_time": "2025-04-21T23:18:18.676969Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Specify the model name for text tokenization\n",
    "text_model_name = \"answerdotai/ModernBERT-base\"\n",
    "\n",
    "# Instantiate the dataset\n",
    "# dataset = DummyMultiModalDataset(num_samples=10, text_model_name=text_model_name)\n",
    "\n",
    "# Get one sample from the dataset\n",
    "sample, label = astrorag_dataset[0]\n",
    "\n",
    "print(\"Text Input IDs:\", sample['text_input_ids'])\n",
    "print(\"Text Attention Mask:\", sample['text_attention_mask'])\n",
    "print(\"Graph Data - x shape:\", sample['graph_data'].x.shape)\n",
    "print(\"Graph Data - edge_index shape:\", sample['graph_data'].edge_index.shape)\n",
    "# print(\"Graph Data - batch shape:\", sample['graph_data'].batch.shape)\n",
    "print(\"Graph Data - y shape:\", sample['graph_data'].y.shape)\n",
    "print(\"Vision Pixel Values:\", sample['pixel_values'])"
   ],
   "id": "3fc1ecfb67633411",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Input IDs: tensor([50281, 31600,   212,   171,   118,   226, 33104,   219, 35458,   123,\n",
      "          168,   216,   224,   167,   228,   227, 49264, 39907,   169,   226,\n",
      "          110,   168,    99,   215, 26532,   227, 26532,   239,  7775,   222,\n",
      "           99,   168,  9223, 16857,   113,   115, 26532,   239, 31600,   212,\n",
      "          171,   118,   226,   187,   187,   158,   239,   231, 32115,   216,\n",
      "        36178,   219,   169,   226,   122,    27,  5987,  1358,    85,    15,\n",
      "         1940,    16,    41,    87, 22351,    45, 13511,    19,    41,    79,\n",
      "          187,   158,   239,   231, 10608,   107,   118, 34123,   168,  9223,\n",
      "        15074,   244,    27,   288,  2140,    18,   296,    15,  2913,    33,\n",
      "         5987,  1358,    85,    15,  1940,    16, 20723,    25,   304, 18933,\n",
      "         2042,    69,   187,   187, 14931,   225,   219, 28774,   107, 34817,\n",
      "        35061,  7775,   217,   223,   167,   242,   220, 10608,   213,   103,\n",
      "        33104,   215, 28774,   219, 44633,   122, 28162, 16857,   115,   107,\n",
      "         2866,  5987,  1358,    85,    15,  1940,    16,    19,  8816, 42636,\n",
      "        42781,  5883,    40, 50282, 50283, 50283, 50283, 50283, 50283, 50283,\n",
      "        50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283,\n",
      "        50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283,\n",
      "        50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283,\n",
      "        50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283,\n",
      "        50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283,\n",
      "        50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283,\n",
      "        50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283,\n",
      "        50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283,\n",
      "        50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283,\n",
      "        50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283,\n",
      "        50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283,\n",
      "        50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283,\n",
      "        50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283, 50283])\n",
      "Text Attention Mask: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Graph Data - x shape: torch.Size([21, 9])\n",
      "Graph Data - edge_index shape: torch.Size([2, 20])\n",
      "Graph Data - y shape: torch.Size([1])\n",
      "Vision Pixel Values: tensor([[[ 0.1451,  0.2078,  0.3176,  ..., -0.6392, -0.5765, -0.5843],\n",
      "         [ 0.4824,  0.3882,  0.4039,  ..., -0.6627, -0.5137, -0.4588],\n",
      "         [ 0.5686,  0.4667,  0.4196,  ..., -0.6549, -0.5373, -0.4824],\n",
      "         ...,\n",
      "         [ 0.5216,  0.5137,  0.5059,  ..., -0.0275, -0.0353,  0.0353],\n",
      "         [ 0.4980,  0.3882,  0.3961,  ..., -0.0275, -0.0431, -0.0118],\n",
      "         [ 0.4275,  0.3176,  0.3255,  ..., -0.0275, -0.0431, -0.0510]],\n",
      "\n",
      "        [[ 0.4353,  0.4745,  0.5608,  ..., -0.6471, -0.5843, -0.5922],\n",
      "         [ 0.6157,  0.5294,  0.5451,  ..., -0.6706, -0.5216, -0.4667],\n",
      "         [ 0.5294,  0.4431,  0.4275,  ..., -0.6627, -0.5451, -0.4902],\n",
      "         ...,\n",
      "         [ 0.5451,  0.5137,  0.4824,  ..., -0.0745, -0.0824, -0.0118],\n",
      "         [ 0.4824,  0.3725,  0.3725,  ..., -0.0745, -0.0902, -0.0588],\n",
      "         [ 0.4039,  0.2941,  0.3020,  ..., -0.0745, -0.0902, -0.1059]],\n",
      "\n",
      "        [[ 0.3804,  0.4275,  0.5529,  ..., -0.6000, -0.5373, -0.5451],\n",
      "         [ 0.5843,  0.5059,  0.5373,  ..., -0.6235, -0.4745, -0.4196],\n",
      "         [ 0.5137,  0.4353,  0.4196,  ..., -0.6078, -0.4980, -0.4431],\n",
      "         ...,\n",
      "         [ 0.4980,  0.4588,  0.4196,  ..., -0.1765, -0.1922, -0.1137],\n",
      "         [ 0.4353,  0.3098,  0.3098,  ..., -0.1686, -0.1843, -0.1529],\n",
      "         [ 0.3569,  0.2471,  0.2392,  ..., -0.1686, -0.1843, -0.1922]]])\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T23:18:23.254381Z",
     "start_time": "2025-04-21T23:18:23.251323Z"
    }
   },
   "cell_type": "code",
   "source": "sample['pixel_values'].shape",
   "id": "ffd659253ed905cb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 224])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T23:18:26.240904Z",
     "start_time": "2025-04-21T23:18:26.114165Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## {\"in_channels\": 9, \"hidden_channels\": 64, \"num_classes\": 2, \"dropout\": 0.2954021195697293, \"lr\": 0.0015804240267104938, \"weight_decay\": 7.64927591337679e-06, \"batch_size\": 128, \"epochs\": 200, \"focal_alpha\": 0.3081135417724518, \"focal_gamma\": 1.3936990483465523}\n",
    "def load_pre_trained_graph_encoder(model_path: str, device: str = \"cpu\") -> UPFDGraphSageNet:\n",
    "    model_file = torch.load(model_path)\n",
    "    state_dict = model_file['model_state_dict']\n",
    "    config = model_file['config']\n",
    "    model = UPFDGraphSageNet(\n",
    "        in_channels=config['in_channels'],\n",
    "        hidden_channels=config['hidden_channels'],\n",
    "        num_classes=config['num_classes'],\n",
    "    )\n",
    "    model.load_state_dict(state_dict)\n",
    "    model = model.to(device)\n",
    "    print(f'MOdel loaded with hidden channels: {model.hidden_channels}')\n",
    "    return model\n",
    "\n",
    "\n",
    "load_pre_trained_graph_encoder(\n",
    "    model_path='/Users/navneet/git/research/swarm-guard/models/graph/graph_encoder.pth')"
   ],
   "id": "b80375e6202d9527",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOdel loaded with hidden channels: 64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "UPFDGraphSageNet(\n",
       "  (conv1): SAGEConv(9, 64, aggr=mean)\n",
       "  (norm1): LayerNorm(64, affine=True, mode=graph)\n",
       "  (conv2): SAGEConv(64, 64, aggr=mean)\n",
       "  (norm2): LayerNorm(64, affine=True, mode=graph)\n",
       "  (conv3): SAGEConv(64, 64, aggr=mean)\n",
       "  (norm3): LayerNorm(64, affine=True, mode=graph)\n",
       "  (classifier): Linear(in_features=64, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T23:18:27.203157Z",
     "start_time": "2025-04-21T23:18:27.187363Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Number of parameters in the model in millions\n",
    "num_params = sum(p.numel() for p in load_pre_trained_graph_encoder(\n",
    "    model_path='/Users/navneet/git/research/swarm-guard/models/graph/graph_encoder.pth').parameters())\n",
    "print(f\"Number of parameters in the model: {num_params / 1e6:.2f}M\")"
   ],
   "id": "3e6f13a83253a2f8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOdel loaded with hidden channels: 64\n",
      "Number of parameters in the model: 0.02M\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T23:18:30.226608Z",
     "start_time": "2025-04-21T23:18:28.034918Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text_encoder = AutoModel.from_pretrained('answerdotai/ModernBERT-base').to(device)\n",
    "graph_encoder = load_pre_trained_graph_encoder(\n",
    "    model_path='/Users/navneet/git/research/swarm-guard/models/graph/graph_encoder.pth',\n",
    "    device=device\n",
    ")\n",
    "vision_encoder = AutoModel.from_pretrained('google/vit-base-patch16-224').to(device)\n",
    "model = MultiModalModelForClassification(\n",
    "    text_encoder=text_encoder,\n",
    "    graph_encoder=graph_encoder,\n",
    "    vision_encoder=vision_encoder,\n",
    "    self_attention_heads=8,\n",
    "    embedding_dim=256,\n",
    "    num_cross_modal_attention_blocks=6,\n",
    "    num_cross_modal_attention_heads=8,\n",
    "    self_attn_ff_dim=512,\n",
    "    num_cross_modal_attention_ff_dim=512,\n",
    "    output_channels=2\n",
    ").to(device)"
   ],
   "id": "aa4f02e8a82436f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOdel loaded with hidden channels: 64\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T23:18:34.395925Z",
     "start_time": "2025-04-21T23:18:34.392199Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Print the number of parameters in the model\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Number of parameters in the model: {num_params / 1e6:.2f}M\")"
   ],
   "id": "605dd715c4f23611",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in the model: 240.58M\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T23:18:49.632637Z",
     "start_time": "2025-04-21T23:18:49.094619Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# forward pass with the sample\n",
    "sample, y = astrorag_dataset[0]\n",
    "text_input_ids = sample['text_input_ids'].unsqueeze(0)\n",
    "text_attention_mask = sample['text_attention_mask'].unsqueeze(0)\n",
    "graph_data = sample['graph_data']\n",
    "#Move the data to the same device as the model\n",
    "text_input_ids = text_input_ids.to(device)\n",
    "text_attention_mask = text_attention_mask.to(device)\n",
    "graph_data.x = graph_data.x.to(device)\n",
    "graph_data.edge_index = graph_data.edge_index.to(device)\n",
    "pixel_values = sample['pixel_values'].unsqueeze(0).to(device)\n",
    "# graph_data.batch = graph_data.batch.to(device)\n",
    "# Perform a forward pass\n",
    "model(text_input_ids, text_attention_mask, graph_data, pixel_values)"
   ],
   "id": "35b46e442e607974",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2046,  0.2821]], device='mps:0', grad_fn=<LinearBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T23:18:52.901163Z",
     "start_time": "2025-04-21T23:18:52.895023Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# print the number of parameters in the model in millions\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Number of parameters in the model: {num_params / 1e6:.2f}M\")\n",
    "\n",
    "# print the number of parameters in the model in millions excluding the text encoder and graph encoder\n",
    "num_params_excluding_encoders = sum(p.numel() for name, p in model.named_parameters() if\n",
    "                                    'text_encoder' not in name and 'graph_encoder' not in name)\n",
    "print(f\"Number of parameters in the model excluding encoders: {num_params_excluding_encoders / 1e6:.2f}M\")"
   ],
   "id": "ae539414b40d3583",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in the model: 240.58M\n",
      "Number of parameters in the model excluding encoders: 91.54M\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T23:19:04.609144Z",
     "start_time": "2025-04-21T23:19:03.219942Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def multimodal_collate_fn(batch):\n",
    "    # Unzip batch elements into data dictionaries and corresponding labels\n",
    "    data_dicts, labels = zip(*batch)\n",
    "\n",
    "    text_input_ids = torch.stack([d['text_input_ids'] for d in data_dicts], dim=0)\n",
    "    text_attention_mask = torch.stack([d['text_attention_mask'] for d in data_dicts], dim=0)\n",
    "\n",
    "    pixel_values = torch.stack([d['pixel_values'] for d in data_dicts], dim=0)\n",
    "\n",
    "    # Create a batched graph using GeoBatch.from_data_list\n",
    "    graph_data = Batch.from_data_list([d['graph_data'] for d in data_dicts])\n",
    "\n",
    "    # Convert labels tuple (of ints) into a tensor.\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    return {\n",
    "        'text_input_ids': text_input_ids,\n",
    "        'text_attention_mask': text_attention_mask,\n",
    "        'graph_data': graph_data,\n",
    "        'pixel_values': pixel_values,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "\n",
    "crieterion = torch.nn.CrossEntropyLoss()\n",
    "data_loader = torch.utils.data.DataLoader(astrorag_dataset, batch_size=2, shuffle=True,\n",
    "                                          collate_fn=multimodal_collate_fn)\n",
    "# iterate through the data loader taking both the features and labels\n",
    "for batch in data_loader:\n",
    "    text_input_ids = batch['text_input_ids']\n",
    "    text_attention_mask = batch['text_attention_mask']\n",
    "    graph_data = batch['graph_data']\n",
    "    labels = batch['labels']\n",
    "    pixel_values = batch['pixel_values']\n",
    "\n",
    "    # Move the data to the same device as the model\n",
    "    text_input_ids = text_input_ids.to(device)\n",
    "    text_attention_mask = text_attention_mask.to(device)\n",
    "    graph_data.x = graph_data.x.to(device)\n",
    "    graph_data.edge_index = graph_data.edge_index.to(device)\n",
    "    graph_data.batch = graph_data.batch.to(device)\n",
    "    pixel_values = pixel_values.to(device)\n",
    "    labels = labels.to(device)\n",
    "    print(f\"Pixel Values Shape: {pixel_values.shape}\")\n",
    "    print(f\"Text Input IDs Shape: {text_input_ids.shape}\")\n",
    "    output = model(text_input_ids, text_attention_mask, graph_data, pixel_values)\n",
    "    loss = crieterion(output, labels)\n",
    "\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    print(\"Labels shape:\", labels.shape)\n",
    "    print(\"Loss:\", loss.item())\n",
    "    break"
   ],
   "id": "a157acaf2969cf7b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixel Values Shape: torch.Size([2, 3, 224, 224])\n",
      "Text Input IDs Shape: torch.Size([2, 280])\n",
      "Output shape: torch.Size([2, 2])\n",
      "Labels shape: torch.Size([2])\n",
      "Loss: 0.5239043831825256\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "76ab5e47d8018c1d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
