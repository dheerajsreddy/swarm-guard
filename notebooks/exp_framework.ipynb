{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T16:51:17.542510Z",
     "start_time": "2025-03-14T16:51:14.292140Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer, AutoFeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "148ff7b2b67ff99d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T16:51:54.841478Z",
     "start_time": "2025-03-14T16:51:54.834733Z"
    }
   },
   "outputs": [],
   "source": [
    "class GraphEncoder(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GraphEncoder, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels) #Maybe GATConv ?\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels) #Maybe GATConv ?\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63202fb135580d25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T17:02:11.599761Z",
     "start_time": "2025-03-14T17:02:11.581282Z"
    }
   },
   "outputs": [],
   "source": [
    "class TriModalBridgeLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.self_attn_text = nn.MultiheadAttention(hidden_dim, num_heads, batch_first=True)\n",
    "        self.self_attn_vision = nn.MultiheadAttention(hidden_dim, num_heads, batch_first=True)\n",
    "        self.self_attn_graph = nn.MultiheadAttention(hidden_dim, num_heads, batch_first=True)\n",
    "        self.cross_attn_text = nn.MultiheadAttention(hidden_dim, num_heads, batch_first=True)\n",
    "        self.cross_attn_vision = nn.MultiheadAttention(hidden_dim, num_heads, batch_first=True)\n",
    "        self.cross_attn_graph = nn.MultiheadAttention(hidden_dim, num_heads, batch_first=True)\n",
    "        self.ff_text = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 4 * hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * hidden_dim, hidden_dim),\n",
    "        )\n",
    "        self.ff_vision = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 4 * hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * hidden_dim, hidden_dim),\n",
    "        )\n",
    "        self.ff_graph = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 4 * hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * hidden_dim, hidden_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, text_emb, vision_emb, graph_emb):\n",
    "        text_emb, _ = self.self_attn_text(text_emb, text_emb, text_emb)\n",
    "        vision_emb, _ = self.self_attn_vision(vision_emb, vision_emb, vision_emb)\n",
    "        graph_emb, _ = self.self_attn_graph(graph_emb, graph_emb, graph_emb)\n",
    "\n",
    "        # (vision, graph), (text,graph), (text, vision) - all 3 combs\n",
    "        #TODO: Or maybe concat all 3 ?\n",
    "        vis_graph_combined = torch.cat([vision_emb, graph_emb], dim=1)\n",
    "        text_emb, _ = self.cross_attn_text(text_emb, vis_graph_combined, vis_graph_combined)\n",
    "        txt_graph_combined = torch.cat([text_emb, graph_emb], dim=1)\n",
    "        vision_emb, _ = self.cross_attn_vision(vision_emb, txt_graph_combined, txt_graph_combined)\n",
    "        txt_vis_combined = torch.cat([text_emb, vision_emb], dim=1)\n",
    "        graph_emb, _ = self.cross_attn_graph(graph_emb, txt_vis_combined, txt_vis_combined)\n",
    "        \n",
    "        text_emb = self.ff_text(text_emb)\n",
    "        vision_emb = self.ff_vision(vision_emb)\n",
    "        graph_emb = self.ff_graph(graph_emb)\n",
    "        \n",
    "        return text_emb, vision_emb, graph_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b44068a5c31569c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T17:03:07.803574Z",
     "start_time": "2025-03-14T17:03:07.787945Z"
    }
   },
   "outputs": [],
   "source": [
    "class TriModalBridgeTower(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        text_model_name: str,\n",
    "        vision_model_name: str,\n",
    "        graph_in_channels: int,\n",
    "        graph_hidden_channels: int,\n",
    "        graph_out_channels: int,\n",
    "        hidden_dim: int,\n",
    "        num_bridge_layers: int = 2,\n",
    "        num_heads: int = 8\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.text_encoder = AutoModel.from_pretrained(text_model_name)\n",
    "        self.vision_encoder = AutoModel.from_pretrained(vision_model_name)\n",
    "        \n",
    "        self.graph_encoder = GraphEncoder(\n",
    "            in_channels=graph_in_channels,\n",
    "            hidden_channels=graph_hidden_channels,\n",
    "            out_channels=graph_out_channels\n",
    "        )\n",
    "        \n",
    "        self.graph_proj = nn.Linear(graph_out_channels, hidden_dim)\n",
    "        \n",
    "        self.bridge_layers = nn.ModuleList([\n",
    "            TriModalBridgeLayer(hidden_dim, num_heads)\n",
    "            for _ in range(num_bridge_layers)\n",
    "        ])\n",
    "        \n",
    "        self.classifier = nn.Linear(hidden_dim, 2)  # genuine vs fake ? Depends on the dataset\n",
    "\n",
    "    def forward(self, text_batch, vision_batch, graph_data):\n",
    "        \"\"\"\n",
    "        text_batch: dict for text model (input_ids, attention_mask, etc.)\n",
    "        vision_batch: dict for vision model (pixel_values, etc.)\n",
    "        graph_data: PyG Data object with .x (node features), .edge_index, etc. (assuming the pyg tutorials)\n",
    "        \"\"\"\n",
    "        \n",
    "        text_outputs = self.text_encoder(**text_batch)\n",
    "        text_emb = text_outputs.last_hidden_state  # (batch_size, seq_len_text, hidden_dim)\n",
    "        \n",
    "        vision_outputs = self.vision_encoder(**vision_batch)\n",
    "        vision_emb = vision_outputs.last_hidden_state  # (batch_size, seq_len_vision, hidden_dim)\n",
    "        \n",
    "        node_emb = self.graph_encoder(graph_data.x, graph_data.edge_index) # I am assuming we have one large graph from the entire twitter dataset i.e the social network graph (probably we generate this using pyspark)\n",
    "        graph_emb = self.graph_proj(node_emb)\n",
    "        if graph_emb.size(0) != batch_size:\n",
    "            graph_emb = graph_emb.expand(batch_size, -1, -1)\n",
    "        \n",
    "        for layer in self.bridge_layers:\n",
    "            text_emb, vision_emb, graph_emb = layer(text_emb, vision_emb, graph_emb)\n",
    "        \n",
    "        # Taking the summaries (CLS tokens)\n",
    "        text_cls = text_emb[:, 0, :]\n",
    "        vision_cls = vision_emb[:, 0, :]\n",
    "        graph_cls = graph_emb[:, 0, :]\n",
    "        \n",
    "        # TODO: Experiment\n",
    "        fused = (text_cls + vision_cls + graph_cls) / 3.0\n",
    "        logits = self.classifier(fused)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5655ca8469f9c42d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T17:03:08.419198Z",
     "start_time": "2025-03-14T17:03:07.937801Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "text_model_name = \"roberta-base\" # I've seen this model usually generalizes well (but then again no free lunch)\n",
    "vision_model_name = \"google/vit-base-patch16-224-in21k\"\n",
    "model = TriModalBridgeTower(\n",
    "    text_model_name=text_model_name,\n",
    "    vision_model_name=vision_model_name,\n",
    "    graph_in_channels=16,\n",
    "    graph_hidden_channels=32,\n",
    "    graph_out_channels=64,\n",
    "    hidden_dim=768,       \n",
    "    num_bridge_layers=2,  #TODO: Experiment, maybe more because we need more hierarchy when we add graph data as well ?\n",
    "    num_heads=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ab86d3bc97293b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T17:03:08.590897Z",
     "start_time": "2025-03-14T17:03:08.426487Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(text_model_name)\n",
    "text_input = [\"Hello world!\", \"This is a test.\"]\n",
    "text_batch = tokenizer(\n",
    "    text_input, padding=True, truncation=True, return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63365fc0e2a3f6f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T17:03:08.605782Z",
     "start_time": "2025-03-14T17:03:08.601237Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = len(text_input)\n",
    "vision_batch = {\n",
    "    \"pixel_values\": torch.randn(batch_size, 3, 224, 224)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c15e2e0239a34ff8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T17:03:08.620177Z",
     "start_time": "2025-03-14T17:03:08.617196Z"
    }
   },
   "outputs": [],
   "source": [
    "x = torch.rand(4, 16) \n",
    "edge_index = torch.tensor([[0, 1, 2, 2],\n",
    "                           [1, 0, 3, 1]], dtype=torch.long)\n",
    "graph_data = Data(x=x, edge_index=edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd15af4073662afc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T17:03:09.739421Z",
     "start_time": "2025-03-14T17:03:08.978996Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n",
      "tensor([[-0.0308, -0.0192],\n",
      "        [-0.0308, -0.0192]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "logits = model(text_batch, vision_batch, graph_data)\n",
    "print(logits.shape) \n",
    "print( logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea85dcef47179bfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
